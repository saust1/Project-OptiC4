{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: statsmodels in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (0.14.1)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: numpy<2,>=1.18 in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from statsmodels) (1.26.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.4 in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.0 in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from statsmodels) (2.1.4)\n",
            "Requirement already satisfied: patsy>=0.5.4 in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from statsmodels) (0.5.4)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from statsmodels) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.3)\n",
            "Requirement already satisfied: six in c:\\users\\steve\\onedrive\\1. baiutek\\project-optic4\\.venv\\lib\\site-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Set max columns to display\n",
        "pd.set_option('display.max_columns', None)\n",
        "from sklearn import preprocessing as pre\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importing CSV files\n",
        "df_CDunit = pd.read_csv(r\"C:\\Users\\steve\\OneDrive\\1. BAIUTEK\\Project-OptiC4\\1 Preprocess\\Continuous Data\\cont_554Data_clean.csv\")\n",
        "df_AlCon = pd.read_csv(r\"C:\\Users\\steve\\OneDrive\\1. BAIUTEK\\Project-OptiC4\\1 Preprocess\\Continuous Data\\cont_425Data_clean.csv\")\n",
        "df_FB554 = pd.read_csv(r\"C:\\Users\\steve\\OneDrive\\1. BAIUTEK\\Project-OptiC4\\1 Preprocess\\Continuous Data\\cont_unitData_clean.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            Butanol\n",
            "count  56390.000000\n",
            "mean       9.541880\n",
            "std       10.186732\n",
            "min        0.000000\n",
            "25%        3.600000\n",
            "50%        5.696670\n",
            "75%       10.636700\n",
            "max       58.353300\n",
            "         425_pct_Al       M_Value    C4_pct_Eth    C4_pct_H2O  \\\n",
            "count  56390.000000  56390.000000  56390.000000  56390.000000   \n",
            "mean       6.111432      3.604672      1.288456     20.731999   \n",
            "std        0.320679      0.172808      0.705936      2.862064   \n",
            "min        0.155707      1.460000      0.086632      7.823650   \n",
            "25%        5.985243      3.507275      0.681256     18.311600   \n",
            "50%        6.129620      3.596210      1.299630     20.677200   \n",
            "75%        6.270160      3.696837      1.727693     22.973050   \n",
            "max        8.058320      5.831770      9.089020     33.941500   \n",
            "\n",
            "       HydWtr_pct_Ammonia  \n",
            "count        56390.000000  \n",
            "mean             0.968764  \n",
            "std              0.145522  \n",
            "min              0.382206  \n",
            "25%              0.877101  \n",
            "50%              0.942325  \n",
            "75%              1.036227  \n",
            "max              1.601090  \n",
            "            DI55152       FC55003       FC55552       FC55569      FFC55553  \\\n",
            "count  56390.000000  56390.000000  56390.000000  56390.000000  56390.000000   \n",
            "mean       0.944869   5942.960010  36147.805685   6596.622988      0.993235   \n",
            "std        0.043178    845.861842   4336.440435    373.930901      0.040359   \n",
            "min        0.814830   2847.690000  18826.300000   5175.290000      0.811764   \n",
            "25%        0.915581   5406.640000  35009.100000   6364.560000      0.969858   \n",
            "50%        0.939644   5958.160000  37897.500000   6576.430000      0.991456   \n",
            "75%        0.966906   6508.520000  38971.575000   6806.467500      1.012158   \n",
            "max        1.065410   9042.150000  46356.900000   8057.390000      1.163840   \n",
            "\n",
            "           FFC55555       LC55555       PI55004       TC55552       TI55021  \n",
            "count  56390.000000  56390.000000  56390.000000  56390.000000  56390.000000  \n",
            "mean       0.774740     58.800001      2.277128    168.524582    223.213627  \n",
            "std        0.023286     10.631938      1.210374     15.046680      8.998408  \n",
            "min        0.694477     27.970000      0.000000    120.198000    203.373000  \n",
            "25%        0.759744     49.874850      1.311962    156.063000    217.874000  \n",
            "50%        0.778625     60.148350      2.036770    171.990000    220.935500  \n",
            "75%        0.790205     67.931700      3.065625    180.628750    224.772000  \n",
            "max        0.855754     86.581300      6.325910    213.521000    256.593000  \n"
          ]
        }
      ],
      "source": [
        "print(df_CDunit.describe())\n",
        "print(df_AlCon.describe())\n",
        "print(df_FB554.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: object\n",
            "Data type for 'Date' column in df_FB554: object\n",
            "Data type for 'Date' column in df_AlCon: object\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: datetime64[ns]\n",
            "Data type for 'Date' column in df_FB554: datetime64[ns]\n",
            "Data type for 'Date' column in df_AlCon: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Date', 'Butanol'], dtype='object')\n",
            "Index(['Date', 'DI55152', 'FC55003', 'FC55552', 'FC55569', 'FFC55553',\n",
            "       'FFC55555', 'LC55555', 'PI55004', 'TC55552', 'TI55021'],\n",
            "      dtype='object')\n",
            "Index(['Date', '425_pct_Al', 'M_Value', 'C4_pct_Eth', 'C4_pct_H2O',\n",
            "       'HydWtr_pct_Ammonia'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df_CDunit.columns)\n",
        "print(df_FB554.columns)\n",
        "print(df_AlCon.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_rolling_average_to_df(df, rolling_size):\n",
        "    # Ensure 'Date' is the index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Apply rolling average to all columns\n",
        "    rolled_df = df.rolling(window=rolling_size, min_periods=1).mean()\n",
        "\n",
        "    # Reset index to make 'Date' a column again\n",
        "    rolled_df = rolled_df.reset_index()\n",
        "\n",
        "    return rolled_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_time_shift_by_hours(df, shift_hours):\n",
        "    \"\"\"\n",
        "    Shifts the DataFrame's datetime index by the specified number of hours.\n",
        "\n",
        "    :param df: DataFrame with 'Date' as its datetime index or column.\n",
        "    :param shift_hours: Number of hours to shift. Can be positive (forward) or negative (backward).\n",
        "    :return: Shifted DataFrame.\n",
        "    \"\"\"\n",
        "    # Convert 'Date' to datetime and set as index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Ensure the index is a DatetimeIndex\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Shift the DataFrame's index by the specified number of hours\n",
        "    df.index = df.index + pd.Timedelta(hours=shift_hours)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Usage Examples\n",
        "# shift_hours_AlCon = 1  # Negative shift for df_AlCon (e.g., -5 hours backward)\n",
        "# shift_hours_FB554 = 5   # Positive shift for df_FB554 (e.g., 5 hours forward)\n",
        "\n",
        "# shifted_df_AlCon = apply_time_shift_by_hours(df_AlCon, shift_hours_AlCon)\n",
        "# print(\"Shifted df_AlCon:\")\n",
        "# print(shifted_df_AlCon.head())\n",
        "\n",
        "# shifted_df_FB554 = apply_time_shift_by_hours(df_FB554, shift_hours_FB554)\n",
        "# print(\"\\nShifted df_FB554:\")\n",
        "# print(shifted_df_FB554.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_df_FB554_to_df_CDunit(df_CDunit, df_FB554):\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "    df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "\n",
        "    df_CDunit = df_CDunit.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_FB554 = df_FB554.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df = pd.merge_asof(df_FB554, df_CDunit, on='Date', direction='nearest')\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def join_df_AlCon_to_combined_df(combined_df, df_AlCon):\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], errors='coerce')\n",
        "    df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n",
        "\n",
        "    combined_df = combined_df.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_AlCon = df_AlCon.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df_all = pd.merge_asof(df_AlCon, combined_df, on='Date', direction='nearest')\n",
        "    \n",
        "    return combined_df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: datetime64[ns]\n",
            "Data type for 'Date' column in df_FB554: datetime64[ns]\n",
            "Data type for 'Date' column in df_AlCon: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iterative merger \n",
        "# avereaged F stat\n",
        "# filtered two closest f stats by whole number\n",
        "# Took the best adj R^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_5160\\1964144134.py:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  results = pd.concat([results, iteration_results_df], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "def apply_negative_shift_hours(shift_hours):\n",
        "    return [-hour for hour in shift_hours]\n",
        "\n",
        "# Rolling sizes ranges\n",
        "rolling_size_CDunit = [8]  # Even rolling sizes from 4 to 10 range(4, 11, 2)\n",
        "rolling_size_FB554 = [4]  # Even rolling sizes from 4 to 10 range(4, 11, 2)\n",
        "rolling_size_AlCon = [2]  # Even rolling sizes from 2 to 30 range(2, 31, 2) \n",
        "\n",
        "# Shift hours ranges\n",
        "shift_hours_AlCon = apply_negative_shift_hours([1])  # Negative shifts from -2 to -8 (range(2, 9, 2)) \n",
        "shift_hours_FB554 = ([1])    # Positive shifts from 2 to 8 range(2, 9, 2)\n",
        "\n",
        "# Precompute rolling averages for each DataFrame and each rolling size\n",
        "precomputed_rolls = {\n",
        "    \"CDunit\": {size: apply_rolling_average_to_df(df_CDunit, size) for size in rolling_size_CDunit},\n",
        "    \"FB554\": {size: apply_rolling_average_to_df(df_FB554, size) for size in rolling_size_FB554},\n",
        "    \"AlCon\": {size: apply_rolling_average_to_df(df_AlCon, size) for size in rolling_size_AlCon}\n",
        "}\n",
        "\n",
        "## Modified process_data function\n",
        "def process_data():\n",
        "    iteration_count = 0\n",
        "    results = pd.DataFrame(columns=['Iteration', 'Rolling Sizes CDunit', 'Rolling Sizes FB554', 'Rolling Sizes AlCon',\n",
        "                                    'Shift Hours AlCon', 'Shift Hours FB554', 'R-squared', 'Adj R-Squared', \n",
        "                                    'F-statistic', 'Prob (F-statistic)'])\n",
        "    all_combined_dfs = []  # List to store combined_df_all for each iteration\n",
        "\n",
        "    for size_CDunit in rolling_size_CDunit:\n",
        "        for size_FB554 in rolling_size_FB554:\n",
        "            for size_AlCon in rolling_size_AlCon:\n",
        "                for shift_hour_AlCon in shift_hours_AlCon:\n",
        "                    for shift_hour_FB554 in shift_hours_FB554:\n",
        "                        iteration_count += 1\n",
        "\n",
        "                        # Retrieve rolled dataframes\n",
        "                        rolled_df_CDunit = precomputed_rolls[\"CDunit\"][size_CDunit]\n",
        "                        rolled_df_FB554 = precomputed_rolls[\"FB554\"][size_FB554]\n",
        "                        rolled_df_AlCon = precomputed_rolls[\"AlCon\"][size_AlCon]\n",
        "\n",
        "                        # Combine df_CDunit and df_FB554 to create combined_df\n",
        "                        combined_df = join_df_FB554_to_df_CDunit(rolled_df_CDunit, rolled_df_FB554)\n",
        "\n",
        "                        # Combine combined_df with rolled_df_AlCon to create combined_df_all\n",
        "                        combined_df_all = join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon)\n",
        "\n",
        "                        # Drop 'Date' column before modeling\n",
        "                        combined_df_all = combined_df_all.drop(columns=['Date'], errors='ignore')\n",
        "\n",
        "                        # Store combined_df_all in the list\n",
        "                        all_combined_dfs.append(combined_df_all)\n",
        "\n",
        "                        # Splitting into train and test\n",
        "                        X = combined_df_all.drop('Butanol', axis=1)\n",
        "                        y = combined_df_all['Butanol']\n",
        "                        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                        # Train model\n",
        "                        model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "                        # Store the results instead of printing\n",
        "                        iteration_results = {\n",
        "                            'Iteration': iteration_count,\n",
        "                            'Rolling Sizes CDunit': size_CDunit,\n",
        "                            'Rolling Sizes FB554': size_FB554,\n",
        "                            'Rolling Sizes AlCon': size_AlCon,\n",
        "                            'Shift Hours AlCon': shift_hour_AlCon,\n",
        "                            'Shift Hours FB554': shift_hour_FB554,\n",
        "                            'R-squared': model.rsquared,\n",
        "                            'Adj R-Squared': model.rsquared_adj,\n",
        "                            'F-statistic': model.fvalue,\n",
        "                            'Prob (F-statistic)': model.f_pvalue\n",
        "                        }\n",
        "                        \n",
        "                        # Convert the dictionary to a DataFrame\n",
        "                        iteration_results_df = pd.DataFrame([iteration_results])\n",
        "                        \n",
        "                        # Concatenate it with the existing DataFrame, wrap iteration_results_df in a list\n",
        "                        results = pd.concat([results, iteration_results_df], ignore_index=True)\n",
        "                        # results = pd.concat(iteration_results_df, ignore_index=True)\n",
        "                        \n",
        "                        # results = pd.concat(iteration_results, ignore_index=True)\n",
        "\n",
        "                        # Print only the iteration count\n",
        "                        print(f\"Iteration: {iteration_count}\")\n",
        "\n",
        "    return results, all_combined_dfs  # Return both results and the list of combined_df_all\n",
        "\n",
        "# Call the function to process and evaluate the data\n",
        "model_results, all_combined_dfs = process_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "model_results.to_csv('model_results2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_df_all = pd.concat(all_combined_dfs)\n",
        "\n",
        "# Assuming combined_df_all is your DataFrame\n",
        "\n",
        "# Calculate the index to split the DataFrame\n",
        "split_index = len(combined_df_all) // 2\n",
        "\n",
        "# Split the DataFrame into two parts\n",
        "df_part1 = combined_df_all.iloc[:split_index]\n",
        "df_part2 = combined_df_all.iloc[split_index:]\n",
        "\n",
        "# Save the two parts to CSV\n",
        "df_part1.to_csv('contData_all_Avg_1o2.csv', index=False)\n",
        "df_part2.to_csv('contData_all_Avg_2o2.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
