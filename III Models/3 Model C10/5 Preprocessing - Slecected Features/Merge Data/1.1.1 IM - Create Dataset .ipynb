{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\austinsh\\AppData\\Local\\Temp\\ipykernel_51588\\1304120184.py:1: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Set max columns to display\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing as pre\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Importing CSV files\n",
        "# df_CDunit = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_554Data_clean.csv')\n",
        "# df_AlCon = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_425Data_clean.csv')\n",
        "# df_FB554 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_unitData_clean.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Importing CSV files\n",
        "# bordeCode directory\n",
        "df_CDunit = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\III Models\\3 Model C10\\5 Preprocessing - Slecected Features\\Continuous Data\\cont_unitData_clean.csv')\n",
        "df_AlCon = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\III Models\\3 Model C10\\5 Preprocessing - Slecected Features\\Continuous Data\\cont_425Data_clean.csv')\n",
        "df_FB554 = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\III Models\\3 Model C10\\5 Preprocessing - Slecected Features\\Continuous Data\\cont_554Data_clean.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            DI55102       DI55152       FC55003       FC55009       FC55552  \\\n",
            "count  53027.000000  53027.000000  53027.000000  53027.000000  53027.000000   \n",
            "mean       0.944303      0.932747   5965.061647    849.267291  35923.727411   \n",
            "std        0.053360      0.030279    873.852122    616.121897   5005.788686   \n",
            "min        0.800002      0.836038   2755.480000      0.000000  18005.000000   \n",
            "25%        0.910721      0.912076   5443.945000    296.416000  34947.100000   \n",
            "50%        0.950220      0.932556   5988.890000    838.219000  37956.500000   \n",
            "75%        0.984479      0.952472   6539.060000   1333.465000  39021.300000   \n",
            "max        1.106550      1.025180   9181.490000   2674.790000  52000.000000   \n",
            "\n",
            "            FC55569       FC55576      FFC55555       LC90366       LC90368  \\\n",
            "count  53027.000000  53027.000000  53027.000000  53027.000000  53027.000000   \n",
            "mean    6596.355555    376.921990      0.772128     46.103877     36.783914   \n",
            "std      389.413386    258.768936      0.022740     28.964180     20.138593   \n",
            "min     5123.870000      0.000000      0.689689      0.000000      0.006367   \n",
            "25%     6359.775000    199.181500      0.758317     20.914450     20.694250   \n",
            "50%     6577.060000    345.814000      0.773698     47.213100     42.069400   \n",
            "75%     6810.550000    561.441000      0.788256     76.966300     51.962850   \n",
            "max     8112.930000   1150.200000      0.854226     87.901600     81.210800   \n",
            "\n",
            "            PI55004       PI55020       TC55552       TI55021  \n",
            "count  53027.000000  53027.000000  53027.000000  53027.000000  \n",
            "mean       2.178256     -1.466144    168.885993    223.009729  \n",
            "std        1.176422      1.130865     15.309291      9.712883  \n",
            "min        0.001366     -4.856830    120.095000    203.525000  \n",
            "25%        1.254095     -2.340425    155.850000    217.508000  \n",
            "50%        1.907760     -1.446300    172.838000    220.246000  \n",
            "75%        2.972360     -0.450221    181.022000    223.843000  \n",
            "max        6.246540      2.030470    202.625000    258.133000  \n",
            "         425_pct_Al    C4_pct_Eth    C4_pct_H2O  HydWtr_pct_Ammonia  \\\n",
            "count  53027.000000  53027.000000  53027.000000        53027.000000   \n",
            "mean       6.143957      1.229628     21.238209            0.970185   \n",
            "std        0.256678      0.733113      2.725474            0.151464   \n",
            "min        5.165900      0.080310      7.667200            0.373360   \n",
            "25%        6.003590      0.625711     18.932350            0.877254   \n",
            "50%        6.138070      1.104550     21.520500            0.941828   \n",
            "75%        6.285380      1.688645     23.298100            1.034630   \n",
            "max        7.136170      9.844330     35.204300            1.605960   \n",
            "\n",
            "        HydWtr_Na2O  \n",
            "count  53027.000000  \n",
            "mean       0.816009  \n",
            "std        0.710180  \n",
            "min        0.000000  \n",
            "25%        0.453602  \n",
            "50%        0.666588  \n",
            "75%        0.985669  \n",
            "max       12.750600  \n",
            "            Decanol\n",
            "count  53027.000000\n",
            "mean       3.171270\n",
            "std        1.651904\n",
            "min        0.000000\n",
            "25%        1.893295\n",
            "50%        3.036670\n",
            "75%        4.213330\n",
            "max        8.510120\n"
          ]
        }
      ],
      "source": [
        "print(df_CDunit.describe())\n",
        "print(df_AlCon.describe())\n",
        "print(df_FB554.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: object\n",
            "Data type for 'Date' column in df_FB554: object\n",
            "Data type for 'Date' column in df_AlCon: object\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: datetime64[ns]\n",
            "Data type for 'Date' column in df_FB554: datetime64[ns]\n",
            "Data type for 'Date' column in df_AlCon: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Date', 'DI55102', 'DI55152', 'FC55003', 'FC55009', 'FC55552',\n",
            "       'FC55569', 'FC55576', 'FFC55555', 'LC90366', 'LC90368', 'PI55004',\n",
            "       'PI55020', 'TC55552', 'TI55021'],\n",
            "      dtype='object')\n",
            "Index(['Date', 'Decanol'], dtype='object')\n",
            "Index(['Date', '425_pct_Al', 'C4_pct_Eth', 'C4_pct_H2O', 'HydWtr_pct_Ammonia',\n",
            "       'HydWtr_Na2O'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df_CDunit.columns)\n",
        "print(df_FB554.columns)\n",
        "print(df_AlCon.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_rolling_average_to_df(df, rolling_size):\n",
        "    # Ensure 'Date' is the index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Apply rolling average to all columns\n",
        "    rolled_df = df.rolling(window=rolling_size, min_periods=1).mean()\n",
        "\n",
        "    # Reset index to make 'Date' a column again\n",
        "    rolled_df = rolled_df.reset_index()\n",
        "\n",
        "    return rolled_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_time_shift_by_hours(df, shift_hours):\n",
        "    \"\"\"\n",
        "    Shifts the DataFrame's datetime index by the specified number of hours.\n",
        "\n",
        "    :param df: DataFrame with 'Date' as its datetime index or column.\n",
        "    :param shift_hours: Number of hours to shift. Can be positive (forward) or negative (backward).\n",
        "    :return: Shifted DataFrame.\n",
        "    \"\"\"\n",
        "    # Convert 'Date' to datetime and set as index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Ensure the index is a DatetimeIndex\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Shift the DataFrame's index by the specified number of hours\n",
        "    df.index = df.index + pd.Timedelta(hours=shift_hours)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Usage Examples\n",
        "# shift_hours_AlCon = 1  # Negative shift for df_AlCon (e.g., -5 hours backward)\n",
        "# shift_hours_FB554 = 5   # Positive shift for df_FB554 (e.g., 5 hours forward)\n",
        "\n",
        "# shifted_df_AlCon = apply_time_shift_by_hours(df_AlCon, shift_hours_AlCon)\n",
        "# print(\"Shifted df_AlCon:\")\n",
        "# print(shifted_df_AlCon.head())\n",
        "\n",
        "# shifted_df_FB554 = apply_time_shift_by_hours(df_FB554, shift_hours_FB554)\n",
        "# print(\"\\nShifted df_FB554:\")\n",
        "# print(shifted_df_FB554.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_df_FB554_to_df_CDunit(df_CDunit, df_FB554):\n",
        "    # Reset index if 'Date' is the index\n",
        "    if df_CDunit.index.name == 'Date':\n",
        "        df_CDunit = df_CDunit.reset_index()\n",
        "    if df_FB554.index.name == 'Date':\n",
        "        df_FB554 = df_FB554.reset_index()\n",
        "\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "    df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "\n",
        "    df_CDunit = df_CDunit.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_FB554 = df_FB554.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df = pd.merge_asof(df_FB554, df_CDunit, on='Date', direction='nearest')\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def join_df_AlCon_to_combined_df(combined_df, df_AlCon):\n",
        "    # Reset index if 'Date' is the index\n",
        "    if combined_df.index.name == 'Date':\n",
        "        combined_df = combined_df.reset_index()\n",
        "    if df_AlCon.index.name == 'Date':\n",
        "        df_AlCon = df_AlCon.reset_index()\n",
        "\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], errors='coerce')\n",
        "    df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n",
        "\n",
        "    combined_df = combined_df.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_AlCon = df_AlCon.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df_all = pd.merge_asof(df_AlCon, combined_df, on='Date', direction='nearest')\n",
        "    \n",
        "    return combined_df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: datetime64[ns]\n",
            "Data type for 'Date' column in df_FB554: datetime64[ns]\n",
            "Data type for 'Date' column in df_AlCon: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_data_limited():\n",
        "    # Apply the specific rolling average directly\n",
        "    rolled_df_CDunit = apply_rolling_average_to_df(df_CDunit, 8)\n",
        "    rolled_df_FB554 = apply_rolling_average_to_df(df_FB554, 4)\n",
        "    rolled_df_AlCon = apply_rolling_average_to_df(df_AlCon, 2)\n",
        "\n",
        "    # Apply the specific time shifts directly\n",
        "    rolled_df_AlCon_shifted = apply_time_shift_by_hours(rolled_df_AlCon, -1) # Assuming apply_time_shift_by_hours handles negative shifts correctly\n",
        "    rolled_df_FB554_shifted = apply_time_shift_by_hours(rolled_df_FB554, 1)\n",
        "\n",
        "    # Combine df_CDunit and df_FB554 to create combined_df\n",
        "    combined_df = join_df_FB554_to_df_CDunit(rolled_df_CDunit, rolled_df_FB554_shifted)\n",
        "\n",
        "    # Combine combined_df with rolled_df_AlCon to create combined_df_all\n",
        "    combined_df_all = join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon_shifted)\n",
        "\n",
        "    # At this point, combined_df_all is the DataFrame with the data processed by the specified shifts and averages\n",
        "    return combined_df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Date  425_pct_Al  C4_pct_Eth  C4_pct_H2O  HydWtr_pct_Ammonia  \\\n",
            "0 2012-05-14 07:00:00     6.26763    2.353670    21.66280            0.519068   \n",
            "1 2012-05-14 08:00:00     6.26798    2.355025    21.66545            0.553096   \n",
            "2 2012-05-14 09:00:00     6.26868    2.357740    21.67070            0.621152   \n",
            "3 2012-05-14 10:00:00     6.26938    2.360455    21.67595            0.667952   \n",
            "4 2012-05-14 11:00:00     6.27008    2.363170    21.68125            0.693497   \n",
            "\n",
            "   HydWtr_Na2O   Decanol   DI55102   DI55152      FC55003    FC55009  \\\n",
            "0     1.088680  0.959336  0.966831  0.938799  6332.030000  2512.3700   \n",
            "1     1.084100  0.959336  0.966831  0.938799  6332.030000  2512.3700   \n",
            "2     1.074945  0.959336  0.966831  0.938799  6332.030000  2512.3700   \n",
            "3     1.065790  0.933602  0.965900  0.937601  6191.103333  2329.9600   \n",
            "4     1.056635  0.907868  0.967110  0.938748  6081.507500  2202.0825   \n",
            "\n",
            "        FC55552      FC55569     FC55576  FFC55555    LC90366    LC90368  \\\n",
            "0  41080.000000  6432.660000  386.948500  0.749378  82.124700  60.124100   \n",
            "1  41080.000000  6432.660000  386.948500  0.749378  82.124700  60.124100   \n",
            "2  41080.000000  6432.660000  386.948500  0.749378  82.124700  60.124100   \n",
            "3  40980.433333  6438.553333  391.853667  0.749920  82.121367  60.174033   \n",
            "4  40679.700000  6438.767500  387.162250  0.750051  82.119700  60.230450   \n",
            "\n",
            "    PI55004   PI55020    TC55552     TI55021  \n",
            "0  0.621513  0.013269  146.99950  215.650000  \n",
            "1  0.621513  0.013269  146.99950  215.650000  \n",
            "2  0.621513  0.013269  146.99950  215.650000  \n",
            "3  0.740749  0.018791  148.86200  215.774333  \n",
            "4  0.897834 -0.021408  150.50375  215.404500  \n"
          ]
        }
      ],
      "source": [
        "# Make sure all your helper functions and initial DataFrames (df_CDunit, df_FB554, df_AlCon) are correctly defined\n",
        "\n",
        "# Now, call the modified process_data function to get the processed DataFrame\n",
        "final_dataset = process_data_limited()\n",
        "\n",
        "# Inspect the final_dataset\n",
        "print(final_dataset.head())  # Print the first few rows to inspect the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "# model_results.to_csv('merged_data'.csv', index=False)\n",
        "                     \n",
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "# df_CD.to_csv(r'C:\\Users\\steve\\OneDrive\\1. BAIUTEK\\Project-OptiC4\\1 Preprocess\\Continuous Data\\contData_all.csv', index=False)            \n",
        "\n",
        "final_dataset.to_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\III Models\\3 Model C10\\5 Preprocessing - Slecected Features\\Merge Data\\merged_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-21 23:18:21.506184\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get the current date and time\n",
        "current_date_time = datetime.now()\n",
        "\n",
        "# Print the current date and time\n",
        "print(current_date_time)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
