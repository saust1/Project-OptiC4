{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Set max columns to display\n",
        "pd.set_option('display.max_columns', None)\n",
        "import numpy as np\n",
        "from sklearn import preprocessing as pre\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing CSV files\n",
        "\n",
        "Switching to a gitHub source for CSV imports\n",
        "\n",
        "\n",
        "url = 'paste_the_raw_github_url_here'\n",
        "\n",
        "df = pd.read_csv(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "########################################################################################################################\n",
        "##############################                                                            ##############################\n",
        "##############################                           IMPORT DATA                      ##############################      \n",
        "##############################                                                            ##############################\n",
        "########################################################################################################################\n",
        "# ABB1_split_1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB1_split_1.csv')\n",
        "# ABB1_split_2 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB1_split_2.csv')\n",
        "# ABB1_1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB1_1.csv')\n",
        "# ABB1_2 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB1_2.csv')\n",
        "# ABB1_4 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB1_4.csv')\n",
        "# ABB1_6 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB1_6.csv')\n",
        "# ABB2_split_1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB2_split_1.csv')\n",
        "# ABB2_split_2 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB2_split_2.csv')\n",
        "# ABB2_1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB2_1.csv')\n",
        "# ABB2_2 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB2_2.csv')\n",
        "# ABB2_4 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB2_4.csv')\n",
        "# ABB2_5 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB2_5.csv')\n",
        "# ABB3 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB3.csv')\n",
        "# ABB3_1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB3_1.csv')\n",
        "# ABB3_2 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB3_2.csv')\n",
        "# ABB4 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB4.csv')\n",
        "# ABB51 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB51.csv')\n",
        "# ABB55 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB55.csv')\n",
        "# ABBp = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABBp.csv')\n",
        "\n",
        "# AlFlow1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/AlFlow1.csv')\n",
        "\n",
        "# DeltaV = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaV.csv')\n",
        "# DeltaV1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaV1.csv')\n",
        "# DeltaV2 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaV2.csv')\n",
        "# DeltaV4 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaV4.csv')\n",
        "# DeltaV5 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaV5.csv')\n",
        "\n",
        "# DeltaVp = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaVp.csv')\n",
        "\n",
        "# DeltaVR_split_1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaVR_split_1.csv')\n",
        "# DeltaVR_split_2 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaVR_split_2.csv')\n",
        "\n",
        "\n",
        "# FC55555_ABB = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/FC55555_ABB.csv')\n",
        "# FC55555_DeltaV = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/FC55555_DeltaV.csv')\n",
        "\n",
        "# LIMS = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/LIM%20-%20554%20-%20Interpolated.csv')\n",
        "# LIMS1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/LIMS%20Actuals/LIMS1.csv')\n",
        "# LIMS2 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/LIMS%20Actuals/LIMS2.csv')\n",
        "# LIMS3 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/LIMS%20Actuals/LIMS3.csv')\n",
        "# LIMS4 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/LIMS%20Actuals/LIMS4.csv')\n",
        "\n",
        "# rC4f_DeltaV = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/rC4f_DeltaV.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ##############   Depricated Files due to Duplicats   ############# #\n",
        "\n",
        "# ABB1_3, #ABB1_5, #ABB5, #ABB11, ABB2_3 ,#ABB22, ABB33, ABB44, ABB56,  rC4f_ABB, LIMS1\n",
        "# ABB1_3 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB1_3.csv')\n",
        "# ABB1_5 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB1_5.csv')\n",
        "# ABB2_3 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB2_3.csv')\n",
        "# ABB5 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB5.csv')\n",
        "# ABB11 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB11.csv')\n",
        "# ABB22 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB22.csv')\n",
        "# ABB33 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB33.csv')\n",
        "# ABB44 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB44.csv')\n",
        "# ABB56 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/ABB56.csv')\n",
        "# DeltaV3 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/DeltaV - XV90911 - HS52551.csv')\n",
        "# LIMS1 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/LIMS1.csv')\n",
        "# rC4f_ABB = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/CSV/PVs/rC4f_ABB.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the code for on-prem data import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ########################################################################################################################\n",
        "# ##############################                                                            ##############################\n",
        "# ##############################                           IMPORT DATA                      ##############################      \n",
        "# ##############################                                                            ##############################\n",
        "# ########################################################################################################################\n",
        "ABB1_split_1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB1_split_1.csv\")\n",
        "ABB1_split_2 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB1_split_2.csv\")\n",
        "ABB1_1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB1_1.csv\")\n",
        "ABB1_2 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB1_2.csv\")\n",
        "ABB1_4 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB1_4.csv\")\n",
        "ABB1_6 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB1_6.csv\")\n",
        "ABB2_split_1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB2_split_1.csv\")\n",
        "ABB2_split_2 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB2_split_2.csv\")\n",
        "ABB2_1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB2_1.csv\")\n",
        "ABB2_2 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB2_2.csv\")\n",
        "ABB2_4 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB2_4.csv\")\n",
        "ABB2_5 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB2_5.csv\")\n",
        "ABB3 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB3.csv\")\n",
        "ABB3_1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB3_1.csv\")\n",
        "ABB3_2 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB3_2.csv\")\n",
        "ABB4 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB4.csv\")\n",
        "ABB51 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB51.csv\")\n",
        "ABB55 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB55.csv\")\n",
        "ABBp = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABBp.csv\")\n",
        "\n",
        "AlFlow1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\AlFlow1.csv\")\n",
        "\n",
        "DeltaV = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaV.csv\")\n",
        "DeltaV1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaV1.csv\")\n",
        "DeltaV2 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaV2.csv\")\n",
        "DeltaV4 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaV4.csv\")\n",
        "DeltaV5 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaV5.csv\")\n",
        "\n",
        "DeltaVp = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaVp.csv\")\n",
        "\n",
        "DeltaVR_split_1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaVR_split_1.csv\")\n",
        "DeltaVR_split_2 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaVR_split_2.csv\")\n",
        "\n",
        "\n",
        "FC55555_ABB = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\FC55555_ABB.csv\")\n",
        "FC55555_DeltaV = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\FC55555_DeltaV.csv\")\n",
        "\n",
        "LIMS = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\LIM - 554 - Interpolated.csv\")\n",
        "LIMS1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\LIMS Actuals\\LIMS1.csv\")\n",
        "LIMS2 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\LIMS Actuals\\LIMS2.csv\")\n",
        "LIMS3 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\LIMS Actuals\\LIMS3.csv\")\n",
        "LIMS4 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\LIMS Actuals\\LIMS4.csv\")\n",
        "\n",
        "\n",
        "rC4f_DeltaV = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\rC4f_DeltaV.csv\")\n",
        "\n",
        "# ABB1_3, #ABB1_5, #ABB5, #ABB11, ABB2_3 ,#ABB22, ABB33, ABB44, ABB56,  rC4f_ABB, LIMS1\n",
        "# ABB1_3 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB1_3.csv\")\n",
        "# ABB1_5 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB1_5.csv\")\n",
        "# ABB2_3 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB2_3.csv\")\n",
        "# ABB5 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB5.csv\")\n",
        "# ABB11 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB11.csv\")\n",
        "# ABB22 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB22.csv\")\n",
        "# ABB33 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB33.csv\")\n",
        "# ABB44 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB44.csv\")\n",
        "# ABB56 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\ABB56.csv\")\n",
        "# DeltaV3 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\DeltaV - XV90911 - HS52551.csv\")\n",
        "# LIMS1 = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\LIMS1.csv\")\n",
        "# rC4f_ABB = pd.read_csv(r\"C:\\Users\\austinsh\\Project-OptiC4\\II Data\\1 Collection\\CSV\\PVs\\rC4f_ABB.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming your dataframes are stored in a list like this:\n",
        "dataframes = [ABB1_split_1, ABB1_split_2, ABB1_1, ABB1_2, ABB1_4, ABB1_6, ABB2_split_1, ABB2_split_2, ABB2_1,\n",
        "                ABB2_4, ABB2_5, ABB3, ABB3_1, ABB3_2, ABB4, ABB51,\n",
        "                ABB55, ABBp, AlFlow1, DeltaV, DeltaV1, DeltaV2, DeltaV4, \n",
        "                DeltaV5, DeltaVp, DeltaVR_split_1, DeltaVR_split_2, LIMS, rC4f_DeltaV, ABB2_2\n",
        "                ]\n",
        "\n",
        "duplicates = []\n",
        "\n",
        "# Compare each dataframe with every other dataframe\n",
        "for i, df1 in enumerate(dataframes):\n",
        "    for j, df2 in enumerate(dataframes):\n",
        "        if i != j and df1.equals(df2):\n",
        "            duplicates.append((i, j))\n",
        "\n",
        "# Report the results\n",
        "if duplicates:\n",
        "    print(\"Duplicate dataframes found:\")\n",
        "    for i, j in duplicates:\n",
        "        print(f\"Dataframe at index {i} is the same as dataframe at index {j}.\")\n",
        "else:\n",
        "    print(\"No duplicate dataframes found.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Concatenate (union) the dataframes\n",
        "df = pd.concat([ABB1_split_1, ABB1_split_2, ABB1_1, ABB1_2, ABB1_4, ABB1_6, \n",
        "                ABB2_split_1, ABB2_split_2, ABB2_1, ABB2_2, ABB2_4, ABB2_5,\n",
        "                ABB3, ABB3_1, ABB3_2, ABB4, \n",
        "                ABB51, ABB55, \n",
        "                ABBp, AlFlow1, \n",
        "                DeltaV, DeltaV1, DeltaV2, DeltaV4, DeltaV5, DeltaVp, DeltaVR_split_1, DeltaVR_split_2,\n",
        "                LIMS, LIMS1, LIMS2, LIMS3, LIMS4,\n",
        "                rC4f_DeltaV], ignore_index=True)\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming df is your DataFrame and 'ID' is the column\n",
        "distinct_ids = df['ID'].unique()\n",
        "\n",
        "print(\"Distinct IDs:\")\n",
        "print(distinct_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "IDs to drop - if needed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ids_to_drop = [\n",
        "\n",
        "# #     # Potential Filtering or Blocking data\n",
        "#         # Atomizer Data\n",
        "#    \"FC52018\", \"TI52014\", \"VI52558B\", \"II52554\", 'DI52018',\n",
        "#         # Stripper pressures\n",
        "#    \"PI55560\", \"PI55004\", \"PI55020\", \"TI40050\", \n",
        "#         # Categorical valve postions\n",
        "#    \"HS90911\", \"HS52551\", 'XV90911',\n",
        "    \n",
        "#     # Not Targeting with the Model\n",
        "#    \"AS74550-4EthanolPP\", \"AS74550-4HexanolPP\", \"AS74550-4OctanolPP\"\n",
        " ]\n",
        "\n",
        "# # Drop rows with specified IDs\n",
        "# df = df[~df['ID'].isin(ids_to_drop)]\n",
        "\n",
        "# # df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Dataset Cleaning\n",
        "\n",
        "## These are the Butanol required Features:\n",
        "\n",
        "### Muted for Decanol Development: \n",
        "Perhaps consider not returning here to run the page. Instead create a function from this page and call it when needed to re-\"preprocess.\" The re-Preprocess ensure minimal data loss due to drops from other features that are not part of the final feature selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ids_to_keep = ['AS73425-3%Al', 'AS74550-4ButanolPP',\n",
        "#                     'FC55569', 'DI55152', 'TC55552',\n",
        "#                     'FC55003', 'LC55555', 'FFC55553',\n",
        "#                     'FFC55555', 'TI55021',\n",
        "#                     'PI55004', 'FC55552',\n",
        "                                 \n",
        "#                'AS74550-25%C2OH', 'AS74550-25%nC4OH',\n",
        "#                'AS74550-25%H2O', 'AS74550-10%M-Value', \n",
        "#                'AS74550-5%pH', 'AS74550-5%NH4OH'\n",
        "               # ]\n",
        "   # 'AS74550-4DecanolPP': 'Decanol',, 'AS74550-5ppmNa2O', 'AS74550-25%nC6OH' 'TC55555',\n",
        "\n",
        "# Keep only rows with specified IDs\n",
        "# df = df[df['ID'].isin(ids_to_keep)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.shape[0])\n",
        "\n",
        "# 3555736 - Before change to include all features (assuming its the butanol only req attributes)\n",
        "# 11313917 - After including all features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y %H:%M', errors='coerce')\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Analysis of the larger yeallow area found an error in the timestamp conversiona\n",
        "## Some timestamps were converted to \"35\" minutes instead of \"25\" minutes\n",
        "\n",
        "#This will be corrected (yyyy-mm-dd hh:35:ss --> yyyy-mm-dd hh:00:ss)\n",
        "# Assuming you have reset the index and kept the timestamp as a separate column named \"Date\"\n",
        "#df['Date'] = df['Date'].apply(lambda dt: dt.replace(minute=0, second=0))\n",
        "\n",
        "# Count rows where minute or second is not 0 before transformation\n",
        "corrections_before = df[df['Date'].apply(lambda dt: dt.minute != 0 or dt.second != 0)].shape[0]\n",
        "\n",
        "# Apply transformation\n",
        "df['Date'] = df['Date'].apply(lambda dt: dt.replace(minute=0, second=0))\n",
        "\n",
        "# The number of corrections is the count of rows where minute or second was not 0\n",
        "print(f\"{corrections_before} corrections were made.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#200208 - before duplicate dfs removed\n",
        "\n",
        "# The data needs to be pivoted to:\n",
        "\n",
        "# Unique row headers by 'Date'\n",
        "# Unique column headers by 'ID'\n",
        "# Values by 'Data'\n",
        "\n",
        "#Previous attempt to pivot indicated Duplicates realtive to Date/ID\n",
        "## These duplicates prevented the pivot\n",
        "\n",
        "#First we must remove the duplicates then Pivot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Identify the duplicates by all columns (Exact duplicates)\n",
        "\n",
        "duplicate_counts = df.groupby(['Date', 'ID', 'Value']).size().reset_index(name='count')\n",
        "duplicates_with_counts = duplicate_counts[duplicate_counts['count'] > 1]\n",
        "print(duplicates_with_counts)\n",
        "\n",
        "#165 10/24/23\n",
        "#1378574 ALL DATA  ['Date', 'ID']\n",
        "#1322098 ALL DATA  ['Date', 'ID', 'Data']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clean = df.drop_duplicates(subset=['Date', 'ID', 'Value'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "########## Actual Duplicates to CSV file ##########\n",
        "\n",
        "# # Find and print duplicates based on 'Date' and 'ID' columns\n",
        "# duplicates = df[df.duplicated(subset=['Date', 'ID'], keep=False)]\n",
        "# print(duplicates)\n",
        "\n",
        "# duplicates.to_csv('duplicates.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Remove the Duplicates\n",
        "\n",
        "#Analysis of duplicates based on ID and Date are not standard across Data for all 332 occurence\n",
        "# Averaging would not be consitent across all duplicate because of the wide spread between various duplicate\n",
        "#Therefore REMOVE all 332 as invalid instead of introducing bias arbitraiarly \n",
        "\n",
        "# Drop duplicates based on 'Date' and 'ID' columns from df_clean\n",
        "print(df_clean.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Identify the duplicates\n",
        "\n",
        "duplicate_counts = df_clean.groupby(['Date', 'ID']).size().reset_index(name='count')\n",
        "duplicates_with_counts = duplicate_counts[duplicate_counts['count'] > 1]\n",
        "print(duplicates_with_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming df_clean is your dataframe\n",
        "df_clean = df_clean.drop_duplicates(subset=['Date', 'ID'], keep='first')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_clean.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Pivot the data\n",
        "df_pivot = df_clean.pivot(index='Date', columns='ID', values='Value')\n",
        "\n",
        "# Reset the index and keep the timestamp as a separate column named \"Date\"\n",
        "df_pivot = df_pivot.reset_index()\n",
        "\n",
        "df_pivot.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_columns = df_pivot.shape[1]\n",
        "print(\"Number of features:\", num_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample DataFrame\n",
        "\n",
        "# Rename the columns\n",
        "column_rename_dict = {\n",
        "    'AS73425-3%Al': '425_pct_Al',\n",
        "    'AS74550-4ButanolPP' : 'Butanol',\n",
        "    'AS74550-4DecanolPP': 'Decanol',\n",
        "    'AS74550-4EthanolPP': 'Ethanol',\n",
        "    'AS74550-4HexanolPP': 'Hexanol',\n",
        "    'AS74550-4OctanolPP': 'Octanol',\n",
        "    'AS74550-25%C2OH': 'C4_pct_Eth',\n",
        "    'AS74550-25%nC6OH': 'C4_pct_Hex',\n",
        "    'AS74550-25%nC4OH': 'C4_pct_But',\n",
        "    'AS74550-25%H2O': 'C4_pct_H2O',\n",
        "    'AS74550-10%M-Value': 'M_Value',\n",
        "    'AS74550-1%Al2O3': 'Al2O3',\n",
        "    'AS74550-5%pH': 'HydWtr_pH',\n",
        "    'AS74550-5ppmNa2O' : 'HydWtr_Na2O',\n",
        "    'AS74550-5%NH4OH': 'HydWtr_pct_Ammonia'\n",
        "}\n",
        "\n",
        "df_pivot = df_pivot.rename(columns=column_rename_dict)\n",
        "\n",
        "df_pivot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pivot.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ids_to_drop = [\n",
        "\n",
        "# # #     # Potential Filtering or Blocking data\n",
        "# #         # Atomizer Data\n",
        "# #    \"FC52018\", \"TI52014\", \"VI52558B\", \"II52554\", 'DI52018',\n",
        "# #         # Stripper pressures\n",
        "# #    \"PI55560\", \"PI55004\", \"PI55020\", \"TI40050\", \n",
        "# #         # Categorical valve postions\n",
        "# #    \"HS90911\", \"HS52551\", 'XV90911',\n",
        "    \n",
        "# #     # Not Targeting with the Model\n",
        "# #    \"AS74550-4EthanolPP\", \"AS74550-4HexanolPP\", \"AS74550-4OctanolPP\"\n",
        "#  ]\n",
        "\n",
        "# # # Drop rows with specified IDs\n",
        "# # df = df[~df['ID'].isin(ids_to_drop)]\n",
        "\n",
        "# # # df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of columns to exclude to run XGboost feature selection\n",
        "exclude_columns = [\n",
        "\t\t\t       'Octanol', \n",
        "\t\t\t       'Hexanol', \n",
        "\t\t\t       'Ethanol', \n",
        "\t\t\t       'Butanol',\n",
        "\t\t\t       'FC52018', \n",
        "\t\t\t       'II52554', \n",
        "\t\t\t       'VI52558B',\n",
        "\t\t\t       'TI52014',    \n",
        "\t\t\t       'TI55013', \n",
        "\t\t\t       'TI55014', \n",
        "\t\t\t       'TI55015', \n",
        "\t\t\t       'TI55016', \n",
        "\t\t\t       'TI55017',\n",
        "\t\t\t       'TC52015',    \n",
        "\t\t\t       'FC55555',      \n",
        "\t\t\t       'PI55560',   \n",
        "\t\t\t       'FYC55553',   \n",
        "\t\t\t       'FC55152',   \n",
        "\t\t\t       'TI55023', \n",
        "\t\t\t       'FC55102', \n",
        "\t\t\t       'FC42428', \n",
        "\t\t\t       'AYC55580',   \n",
        "\t\t\t       'TC55566', \n",
        "\t\t\t       'LC55555',  \n",
        "\t\t\t       'TC55553',\n",
        "                    'TI40050',\n",
        "                    'TC55555',\n",
        "                    'LC55568',\n",
        "                    'LC55557',\n",
        "                    'Al2O3',\n",
        "                    'LC55553',\n",
        "                    'M_Value',\n",
        "                    'FFC55553',\n",
        "                    'DI55580',\n",
        "                    'Al2O3',\n",
        "                    'C4_pct_Hex',\n",
        "                    'LC52572',\n",
        "       \n",
        "#        # Sample data of the same time introcuces Data leakage\n",
        "#        'Octanol', 'Hexanol', 'Ethanol', 'Decanol',\n",
        "\n",
        "#        # Niro Functions\n",
        "#        'TC52015', 'FC52018', 'II52554', 'VI52558B',\n",
        "\n",
        "#        # Grabbed and nothing indeicates requirement\n",
        "#        'C4_pct_Hex', 'HydWtr_Na2O'\n",
        "\n",
        "\n",
        "#       #  # Highly correlated to TC's\n",
        "#       #  # \"Normally the Exctraction temperature is run within a few degrees of the hydrolysis temperature.\"(Alumina Unit Effect p10) \n",
        "#       #  'TI52014', 'TI55013', 'TI55014', 'TI55015', 'TI55016', 'TI55017', 'TI55023', 'TI40050',\n",
        "\n",
        "           \n",
        "#       #  # .9 to .8 Correlations\n",
        "#       #  'FC55555',  # First Drop - Highest correlated variable    #Almost 1 with FC55552 - as determined by Ratio controller    \n",
        "#       #  'PI55560',   # Exlude middle pressure - Include -> PI5500: btm pressure \n",
        "#       #  'FYC55553',  # Calculates by other varibales (inherently correlated) - Include -> FC55552 - leading influecner \n",
        "#       #  'FC55152', 'FC55102', # Both are deritives of FC55552\n",
        "#       #  'AYC55580',  # Is calculated - DI55580 measures the process directly\n",
        "       \n",
        "#       #  # .7 Correlations\n",
        "#       #  # FC55552 --> FC42428 = 0.70526 / not comfortable droping either at this time\n",
        "\n",
        "#       #  # .6 Correlations\n",
        "#       #  'TC55566',  # Slurry coolers are a Downstream requirement - TC55552 occurs upstream (DC-551 ALKOX  FD PREHEAT)\n",
        "\n",
        "#       #  # .5 Correlations\n",
        "#       #  'FC55009' # Receives its SP from TC55553\n",
        "\n",
        "                   ]\n",
        "\n",
        "# Create a new DataFrame without the excluded columnsd\n",
        "df_pivot = df_pivot.drop(columns=exclude_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# To Address the NaNs ....................................................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Count NaNs in each row\n",
        "nan_counts_per_row = df_pivot.isnull().sum(axis=1)\n",
        "\n",
        "# Frequency distribution of NaN counts\n",
        "frequency_distribution = nan_counts_per_row.value_counts().sort_index()\n",
        "\n",
        "print(frequency_distribution)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Count of rows where all values are NaN\n",
        "all_nan_rows_count = df_pivot.isnull().all(axis=1).sum()\n",
        "\n",
        "# Total number of rows in pivoted_df\n",
        "total_rows = len(df_pivot)\n",
        "\n",
        "# Percentage of rows that are completely NaN\n",
        "all_nan_percentage = (all_nan_rows_count / total_rows) * 100\n",
        "\n",
        "print(f\"Number of rows with NaN for every column: {all_nan_rows_count}\")\n",
        "print(f\"Total number of rows: {total_rows}\")\n",
        "print(f\"Percentage of rows with NaN for every column: {all_nan_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculate the statistics for each column in df_pivot\n",
        "total_rows = len(df_pivot)\n",
        "non_nan_count = df_pivot.count()\n",
        "nan_count = df_pivot.isnull().sum()\n",
        "nan_percentage = (nan_count / total_rows) * 100\n",
        "\n",
        "# Store these in a DataFrame\n",
        "nan_stats = pd.DataFrame({\n",
        "    'ID': df_pivot.columns,\n",
        "    'total_rows': total_rows,\n",
        "    'non_nan_count': non_nan_count.values,\n",
        "    'nan_count': nan_count.values,\n",
        "    'nan_percentage': nan_percentage.values\n",
        "})\n",
        "\n",
        "print(nan_stats)\n",
        "\n",
        "## Plot\n",
        "#nan_stats.set_index('ID')['nan_percentage'].plot(kind='bar')\n",
        "#plt.ylabel('Percentage of NaNs')\n",
        "#plt.title('Percentage of NaNs in each column')\n",
        "#plt.show()\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create a binary matrix to represent NaNs (1 for NaN, 0 for a number)\n",
        "nan_matrix = np.where(df_pivot.isna(), 1, 0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 10))  # Adjust the size as needed\n",
        "cmap = plt.get_cmap('viridis', 2)  # We use a colormap that differentiates between 0 and 1 clearly\n",
        "\n",
        "# Plotting heatmap\n",
        "cax = ax.imshow(nan_matrix, cmap=cmap, aspect='auto')\n",
        "\n",
        "# Adding colorbar for reference\n",
        "plt.colorbar(cax, ticks=[0, 1], label='NaN Presence')\n",
        "plt.title(\"NaN Presence in Data (1 for NaN, 0 for Not NaN)\", pad=20)\n",
        "\n",
        "# To make the columns readable and vertical\n",
        "plt.xticks(range(df_pivot.shape[1]), df_pivot.columns, rotation=90)  # rotation set to 90 for vertical labels\n",
        "\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Row Index')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a threshold for the percentage of NaN values\n",
        "threshold = 40\n",
        "\n",
        "# Drop columns with more than 'threshold' percentage of NaN values\n",
        "nan_stats_filtered = nan_stats[nan_stats['nan_percentage'] <= threshold]\n",
        "\n",
        "# Get the list of columns to keep\n",
        "columns_to_keep = nan_stats_filtered['ID'].tolist()\n",
        "\n",
        "\n",
        "df_pivot_50pct_nan_drop = df_pivot.drop(columns=columns_to_keep)\n",
        "\n",
        "\n",
        "# Filter your original DataFrame (df_pivot) to keep only the selected columns\n",
        "df_pivot = df_pivot[columns_to_keep]\n",
        "\n",
        "# Now, df_pivot_filtered contains only columns with less than or equal to 50% NaN values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dropped \n",
        "Because there wasn't enough viable data to process\n",
        "\n",
        "Consider revisting source data, if final model inadequate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Describe rationale in report WHY WASN'T FURTHER ANALYSIS CONDUCTED?\n",
        "df_pivot_50pct_nan_drop.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Columns to drop for df_toomanyNaN\n",
        "#columns_toomanyNaN = ['HS52551', 'XV90911']\n",
        "# 'TI55565A',\n",
        "\n",
        "# Columns to drop from df_pivot\n",
        "#columns_drop_pivot = ['DI52018', 'FC52018', 'FI55580', 'LC52572', 'TC52015', 'TC55566']\n",
        "\n",
        "# 1. Create df_toomanyNaN with the specified columns\n",
        "#df_toomanyNaN = df_pivot[columns_toomanyNaN].copy()\n",
        "\n",
        "# 2. Create df_suspect which is df_pivot minus the problematic columns\n",
        "#df_suspect = df_pivot.drop(columns=columns_toomanyNaN).copy()\n",
        "\n",
        "# 3. Drop the columns from df_pivot\n",
        "#df_pivot = df_pivot.drop(columns=columns_toomanyNaN)\n",
        "# + columns_drop_pivot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create a binary matrix to represent NaNs (1 for NaN, 0 for a number)\n",
        "nan_matrix = np.where(df_pivot.isna(), 1, 0)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 10))  # Adjust the size as needed\n",
        "cmap = plt.get_cmap('viridis', 2)  # We use a colormap that differentiates between 0 and 1 clearly\n",
        "\n",
        "# Plotting heatmap\n",
        "cax = ax.imshow(nan_matrix, cmap=cmap, aspect='auto')\n",
        "\n",
        "# Adding colorbar for reference\n",
        "plt.colorbar(cax, ticks=[0, 1], label='NaN Presence')\n",
        "plt.title(\"NaN Presence in Data (1 for NaN, 0 for Not NaN)\", pad=20)\n",
        "\n",
        "# To make the columns readable and vertical\n",
        "plt.xticks(range(df_pivot.shape[1]), df_pivot.columns, rotation=90)  # rotation set to 90 for vertical labels\n",
        "\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Row Index')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Calculate the statistics for each column in df_pivot\n",
        "total_rows = len(df_pivot)\n",
        "non_nan_count = df_pivot.count()\n",
        "nan_count = df_pivot.isnull().sum()\n",
        "nan_percentage = (nan_count / total_rows) * 100\n",
        "\n",
        "# Store these in a DataFrame\n",
        "nan_stats = pd.DataFrame({\n",
        "    'ID': df_pivot.columns,\n",
        "    'total_rows': total_rows,\n",
        "    'non_nan_count': non_nan_count.values,\n",
        "    'nan_count': nan_count.values,\n",
        "    'nan_percentage': nan_percentage.values\n",
        "})\n",
        "\n",
        "print(nan_stats)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Analysis of remaining NaNs suggests minimal impact - REMOVE\n",
        "\n",
        "df_CD = df_pivot.dropna().copy()\n",
        "\n",
        "# Calculate the statistics for each column in df_pivot\n",
        "total_rows = len(df_CD)\n",
        "non_nan_count = df_CD.count()\n",
        "nan_count = df_CD.isnull().sum()\n",
        "nan_percentage = (nan_count / total_rows) * 100\n",
        "\n",
        "# Store these in a DataFrame\n",
        "nan_stats = pd.DataFrame({\n",
        "    'ID': df_CD.columns,\n",
        "    'total_rows': total_rows,\n",
        "    'non_nan_count': non_nan_count.values,\n",
        "    'nan_count': nan_count.values,\n",
        "    'nan_percentage': nan_percentage.values\n",
        "})\n",
        "\n",
        "print(nan_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 92515 Records - All duplicates removed - All NaNs removed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(df_CD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(df_CD.head())\n",
        "\n",
        "# https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/contData_all.csv\n",
        "\n",
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "# df_CD.to_csv(r'C:\\Users\\steve\\OneDrive\\1. BAIUTEK\\Project-OptiC4\\1 Preprocess\\Continuous Data\\contData_all.csv', index=False)\n",
        "\n",
        "# Board workstation local copy\n",
        "df_CD.to_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\III Models\\3 Model C10\\5 Preprocessing - Slecected Features\\Continuous Data\\contData_all.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_CD.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Ensure 'Date' is in datetime format\n",
        "# df_CD['Date'] = pd.to_datetime(df_CD['Date'])\n",
        "\n",
        "# # Calculate the difference between current and previous date-time\n",
        "# df_CD['Time_Diff'] = df_CD['Date'].diff()\n",
        "\n",
        "# # Find where 'Time_Diff' is greater than 1 hour\n",
        "# gaps_over_one_hour = df_CD[df_CD['Time_Diff'] > pd.Timedelta(hours=1)]\n",
        "\n",
        "# gaps_over_one_hour\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_columns = df_CD.shape[1]\n",
        "print(\"Number of features:\", num_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After all Duplicates, NaNs, and inadequate record quantaties removed\n",
        "\n",
        "57 Features remain\n",
        "\n",
        "Outliers are to be addressed next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get the current date and time\n",
        "current_date_time = datetime.now()\n",
        "\n",
        "# Print the current date and time\n",
        "print(current_date_time)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
