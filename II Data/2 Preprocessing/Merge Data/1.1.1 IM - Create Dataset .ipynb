{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Set max columns to display\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing as pre\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Importing CSV files\n",
        "# df_CDunit = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_554Data_clean.csv')\n",
        "# df_AlCon = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_425Data_clean.csv')\n",
        "# df_FB554 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_unitData_clean.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Importing CSV files\n",
        "# bordeCode directory\n",
        "df_CDunit = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\II Data\\2 Preprocessing\\Continuous Data\\cont_unitData_clean.csv')\n",
        "df_AlCon = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\II Data\\2 Preprocessing\\Continuous Data\\cont_425Data_clean.csv')\n",
        "df_FB554 = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\II Data\\2 Preprocessing\\Continuous Data\\cont_554Data_clean.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(df_CDunit.describe())\n",
        "print(df_AlCon.describe())\n",
        "print(df_FB554.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_CDunit.columns)\n",
        "print(df_FB554.columns)\n",
        "print(df_AlCon.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_rolling_average_to_df(df, rolling_size):\n",
        "    # Ensure 'Date' is the index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Apply rolling average to all columns\n",
        "    rolled_df = df.rolling(window=rolling_size, min_periods=1).mean()\n",
        "\n",
        "    # Reset index to make 'Date' a column again\n",
        "    rolled_df = rolled_df.reset_index()\n",
        "\n",
        "    return rolled_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_time_shift_by_hours(df, shift_hours):\n",
        "    \"\"\"\n",
        "    Shifts the DataFrame's datetime index by the specified number of hours.\n",
        "\n",
        "    :param df: DataFrame with 'Date' as its datetime index or column.\n",
        "    :param shift_hours: Number of hours to shift. Can be positive (forward) or negative (backward).\n",
        "    :return: Shifted DataFrame.\n",
        "    \"\"\"\n",
        "    # Convert 'Date' to datetime and set as index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Ensure the index is a DatetimeIndex\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Shift the DataFrame's index by the specified number of hours\n",
        "    df.index = df.index + pd.Timedelta(hours=shift_hours)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Usage Examples\n",
        "# shift_hours_AlCon = 1  # Negative shift for df_AlCon (e.g., -5 hours backward)\n",
        "# shift_hours_FB554 = 5   # Positive shift for df_FB554 (e.g., 5 hours forward)\n",
        "\n",
        "# shifted_df_AlCon = apply_time_shift_by_hours(df_AlCon, shift_hours_AlCon)\n",
        "# print(\"Shifted df_AlCon:\")\n",
        "# print(shifted_df_AlCon.head())\n",
        "\n",
        "# shifted_df_FB554 = apply_time_shift_by_hours(df_FB554, shift_hours_FB554)\n",
        "# print(\"\\nShifted df_FB554:\")\n",
        "# print(shifted_df_FB554.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_df_FB554_to_df_CDunit(df_CDunit, df_FB554):\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "    df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "\n",
        "    df_CDunit = df_CDunit.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_FB554 = df_FB554.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df = pd.merge_asof(df_FB554, df_CDunit, on='Date', direction='nearest')\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def join_df_AlCon_to_combined_df(combined_df, df_AlCon):\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], errors='coerce')\n",
        "    df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n",
        "\n",
        "    combined_df = combined_df.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_AlCon = df_AlCon.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df_all = pd.merge_asof(df_AlCon, combined_df, on='Date', direction='nearest')\n",
        "    \n",
        "    return combined_df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_data_limited():\n",
        "    # Apply the specific rolling average directly\n",
        "    rolled_df_CDunit = apply_rolling_average_to_df(df_CDunit, 8)\n",
        "    rolled_df_FB554 = apply_rolling_average_to_df(df_FB554, 4)\n",
        "    rolled_df_AlCon = apply_rolling_average_to_df(df_AlCon, 2)\n",
        "\n",
        "    # Apply the specific time shifts directly\n",
        "    rolled_df_AlCon_shifted = apply_time_shift_by_hours(rolled_df_AlCon, -1) # Assuming apply_time_shift_by_hours handles negative shifts correctly\n",
        "    rolled_df_FB554_shifted = apply_time_shift_by_hours(rolled_df_FB554, 1)\n",
        "\n",
        "    # Combine df_CDunit and df_FB554 to create combined_df\n",
        "    combined_df = join_df_FB554_to_df_CDunit(rolled_df_CDunit, rolled_df_FB554_shifted)\n",
        "\n",
        "    # Combine combined_df with rolled_df_AlCon to create combined_df_all\n",
        "    combined_df_all = join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon_shifted)\n",
        "\n",
        "    # At this point, combined_df_all is the DataFrame with the data processed by the specified shifts and averages\n",
        "    return combined_df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make sure all your helper functions and initial DataFrames (df_CDunit, df_FB554, df_AlCon) are correctly defined\n",
        "\n",
        "# Now, call the modified process_data function to get the processed DataFrame\n",
        "final_dataset = process_data_limited()\n",
        "\n",
        "# Inspect the final_dataset\n",
        "print(final_dataset.head())  # Print the first few rows to inspect the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def apply_negative_shift_hours(shift_hours):\n",
        "#     return [-hour for hour in shift_hours]\n",
        "\n",
        "# # Make sure to have your DataFrames: df_CDunit, df_FB554, df_AlCon ready\n",
        "\n",
        "# # Apply the specific rolling average directly\n",
        "# rolled_df_CDunit = apply_rolling_average_to_df(df_CDunit, 8)\n",
        "# rolled_df_FB554 = apply_rolling_average_to_df(df_FB554, 4)\n",
        "# rolled_df_AlCon = apply_rolling_average_to_df(df_AlCon, 2)\n",
        "\n",
        "# # Apply the specific time shifts directly\n",
        "# # Note: Since you're applying a negative shift to AlCon and a positive shift to FB554, ensure this logic is correctly handled in your apply_time_shift_by_hours function\n",
        "# rolled_df_AlCon_shifted = apply_time_shift_by_hours(rolled_df_AlCon, -1) # apply_negative_shift_hours already applied in specifying -1\n",
        "# rolled_df_FB554_shifted = apply_time_shift_by_hours(rolled_df_FB554, 1)\n",
        "\n",
        "# # Merge the rolled and shifted DataFrames accordingly\n",
        "# combined_df = join_df_FB554_to_df_CDunit(rolled_df_CDunit, rolled_df_FB554_shifted)\n",
        "# combined_df_all = join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon_shifted)\n",
        "\n",
        "# # Now, combined_df_all is the DataFrame you're interested in\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "# def apply_negative_shift_hours(shift_hours):\n",
        "#     return [-hour for hour in shift_hours]\n",
        "\n",
        "# # Rolling sizes ranges\n",
        "# rolling_size_CDunit = [8]  # Even rolling sizes from 4 to 10 range(4, 11, 2)\n",
        "# rolling_size_FB554 = [4]  # Even rolling sizes from 4 to 10 range(4, 11, 2)\n",
        "# rolling_size_AlCon = [2]  # Even rolling sizes from 2 to 30 range(2, 31, 2) \n",
        "\n",
        "# # Shift hours ranges\n",
        "# shift_hours_AlCon = apply_negative_shift_hours([1])  # Negative shifts from -2 to -8 (range(2, 9, 2)) \n",
        "# shift_hours_FB554 = ([1])    # Positive shifts from 2 to 8 range(2, 9, 2)\n",
        "\n",
        "# # Precompute rolling averages for each DataFrame and each rolling size\n",
        "# precomputed_rolls = {\n",
        "#     \"CDunit\": {size: apply_rolling_average_to_df(df_CDunit, size) for size in rolling_size_CDunit},\n",
        "#     \"FB554\": {size: apply_rolling_average_to_df(df_FB554, size) for size in rolling_size_FB554},\n",
        "#     \"AlCon\": {size: apply_rolling_average_to_df(df_AlCon, size) for size in rolling_size_AlCon}\n",
        "# }\n",
        "\n",
        "# results = pd.DataFrame()\n",
        "\n",
        "# ## Modified process_data function\n",
        "# def process_data():\n",
        "#     iteration_count = 0\n",
        "#     results = pd.DataFrame(columns=['Iteration', 'Rolling Sizes CDunit', 'Rolling Sizes FB554', 'Rolling Sizes AlCon',\n",
        "#                                     'Shift Hours AlCon', 'Shift Hours FB554', 'R-squared', 'Adj R-Squared', \n",
        "#                                     'F-statistic', 'Prob (F-statistic)'])\n",
        "\n",
        "#     for size_CDunit in rolling_size_CDunit:\n",
        "#         for size_FB554 in rolling_size_FB554:\n",
        "#             for size_AlCon in rolling_size_AlCon:\n",
        "#                 for shift_hour_AlCon in shift_hours_AlCon:\n",
        "#                     for shift_hour_FB554 in shift_hours_FB554:\n",
        "#                         iteration_count += 1\n",
        "\n",
        "#                         # Retrieve rolled dataframes\n",
        "#                         rolled_df_CDunit = precomputed_rolls[\"CDunit\"][size_CDunit]\n",
        "#                         rolled_df_FB554 = precomputed_rolls[\"FB554\"][size_FB554]\n",
        "#                         rolled_df_AlCon = precomputed_rolls[\"AlCon\"][size_AlCon]\n",
        "\n",
        "#                         # Combine df_CDunit and df_FB554 to create combined_df\n",
        "#                         combined_df = join_df_FB554_to_df_CDunit(rolled_df_CDunit, rolled_df_FB554)\n",
        "\n",
        "#                         # Combine combined_df with rolled_df_AlCon to create combined_df_all\n",
        "#                         combined_df_all = join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon)\n",
        "\n",
        "#                         # Drop 'Date' column before modeling\n",
        "#                         combined_df_all = combined_df_all.drop(columns=['Date'], errors='ignore')\n",
        "\n",
        "#                         # Splitting into train and test\n",
        "#                         X = combined_df_all.drop('Butanol', axis=1)\n",
        "#                         y = combined_df_all['Butanol']\n",
        "#                         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#                         # Train model\n",
        "#                         model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "#                         # Store the results instead of printing\n",
        "#                         iteration_results = {\n",
        "#                             'Iteration': iteration_count,\n",
        "#                             'Rolling Sizes CDunit': size_CDunit,\n",
        "#                             'Rolling Sizes FB554': size_FB554,\n",
        "#                             'Rolling Sizes AlCon': size_AlCon,\n",
        "#                             'Shift Hours AlCon': shift_hour_AlCon,\n",
        "#                             'Shift Hours FB554': shift_hour_FB554,\n",
        "#                             'R-squared': model.rsquared,\n",
        "#                             'Adj R-Squared': model.rsquared_adj,\n",
        "#                             'F-statistic': model.fvalue,\n",
        "#                             'Prob (F-statistic)': model.f_pvalue\n",
        "#                         }\n",
        "#                         # 1 results = results.append(iteration_results, ignore_index=True)?\n",
        "#                         # 2 results = pd.concat([results, iteration_results], ignore_index=True)\n",
        "#                         # iteration_results_df = pd.DataFrame([iteration_results])\n",
        "#                         # results = pd.concat([results], ignore_index=True) #, iteration_results_df\n",
        "#                         iteration_results_df = pd.DataFrame([iteration_results])\n",
        "#                         results = pd.concat([results, iteration_results_df], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "#                         # Print only the iteration count\n",
        "#                         print(f\"Iteration: {iteration_count}\")\n",
        "\n",
        "#     return results\n",
        "\n",
        "# # Call the function to process and evaluate the data\n",
        "# model_results = process_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "# model_results.to_csv('merged_data'.csv', index=False)\n",
        "                     \n",
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "# df_CD.to_csv(r'C:\\Users\\steve\\OneDrive\\1. BAIUTEK\\Project-OptiC4\\1 Preprocess\\Continuous Data\\contData_all.csv', index=False)            \n",
        "\n",
        "model_results.to_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\II Data\\2 Preprocessing\\Merge Data\\merged_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get the current date and time\n",
        "current_date_time = datetime.now()\n",
        "\n",
        "# Print the current date and time\n",
        "print(current_date_time)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
