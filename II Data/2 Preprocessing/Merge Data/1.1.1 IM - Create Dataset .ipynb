{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\austinsh\\AppData\\Local\\Temp\\ipykernel_51840\\1304120184.py:1: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Set max columns to display\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import preprocessing as pre\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Importing CSV files\n",
        "# df_CDunit = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_554Data_clean.csv')\n",
        "# df_AlCon = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_425Data_clean.csv')\n",
        "# df_FB554 = pd.read_csv('https://raw.githubusercontent.com/saust1/Project-OptiC4/main/1%20Preprocess/Continuous%20Data/cont_unitData_clean.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Importing CSV files\n",
        "# bordeCode directory\n",
        "df_CDunit = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\II Data\\2 Preprocessing\\Continuous Data\\cont_unitData_clean.csv')\n",
        "df_AlCon = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\II Data\\2 Preprocessing\\Continuous Data\\cont_425Data_clean.csv')\n",
        "df_FB554 = pd.read_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\II Data\\2 Preprocessing\\Continuous Data\\cont_554Data_clean.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           AYC55580       DI55102       DI55152       DI55580       FC42428  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean      11.106674      0.944623      0.932484      0.995797  36811.095494   \n",
            "std        4.087796      0.051776      0.029015      0.046053   4311.627143   \n",
            "min        0.000000      0.800007      0.837832      0.853967  19249.500000   \n",
            "25%        9.924740      0.911098      0.913068      0.965977  35739.700000   \n",
            "50%       10.786900      0.949496      0.932305      0.997192  38146.000000   \n",
            "75%       13.196700      0.984039      0.951719      1.029190  39516.800000   \n",
            "max       20.000000      1.052060      1.020350      1.098610  49959.600000   \n",
            "\n",
            "            FC52018       FC55003       FC55009       FC55102       FC55152  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean   32866.825818   5994.183724    845.358602  44806.974341  40473.976083   \n",
            "std     6142.280506    824.261804    610.986652   5949.433189   5335.439123   \n",
            "min    12699.500000   2873.730000      0.000000  21122.000000  19227.500000   \n",
            "25%    29068.700000   5496.925000    291.609000  42198.900000  38570.650000   \n",
            "50%    32985.500000   6016.260000    840.731000  46087.100000  41596.000000   \n",
            "75%    36623.500000   6542.060000   1321.545000  48605.000000  43766.200000   \n",
            "max    50000.000000   9070.260000   2661.680000  62848.200000  57371.200000   \n",
            "\n",
            "            FC55552       FC55555       FC55569       FC55576      FFC55553  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean   36493.996661  28208.902614   6599.391815    370.830216      0.998758   \n",
            "std     4137.140382   3340.330378    372.658838    262.256204      0.036322   \n",
            "min    18969.100000  14364.900000   5119.710000      0.000000      0.811764   \n",
            "25%    35401.350000  27329.850000   6364.805000    182.811500      0.974867   \n",
            "50%    37985.300000  29274.100000   6578.840000    341.189000      0.998093   \n",
            "75%    39035.200000  30347.650000   6810.440000    557.054000      1.019000   \n",
            "max    52000.000000  34638.100000   8114.690000   1143.840000      1.176560   \n",
            "\n",
            "           FFC55555      FYC55553       II52554       LC52572       LC55553  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean       0.772741  36402.826952    170.599779     63.161923     62.999857   \n",
            "std        0.021794   4005.413257     28.885462      2.611648      7.290771   \n",
            "min        0.696182  18434.200000     77.923500     53.656300     41.398300   \n",
            "25%        0.759202  35277.900000    149.521500     61.812500     58.662150   \n",
            "50%        0.774090  37641.100000    167.449000     63.556800     65.211200   \n",
            "75%        0.788650  38890.300000    190.134000     64.980550     68.068650   \n",
            "max        0.847795  46156.700000    261.639000     72.387600     82.753500   \n",
            "\n",
            "            LC55555       LC55557       LC55568       LC90366       LC90368  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean      56.453715     69.400958     41.197444     47.683837     35.824183   \n",
            "std       10.004467      2.890560      1.555107     28.772344     20.401483   \n",
            "min       27.970000     60.183500     32.042400      0.000000      0.006367   \n",
            "25%       46.521350     66.841000     40.846150     23.318200     18.997750   \n",
            "50%       59.831300     70.055500     41.269800     49.597200     40.975900   \n",
            "75%       64.954550     71.685400     41.639150     78.308300     51.463000   \n",
            "max       79.033400     78.119700     49.650600     87.901600     81.210800   \n",
            "\n",
            "            PI55004       PI55020       PI55560       TC52015       TC55552  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean       2.183557     -1.493130      1.018803     74.558815    168.669383   \n",
            "std        1.139412      1.129831      0.981951     16.293286     15.580895   \n",
            "min        0.001366     -4.856830     -1.748120     27.143100    119.683000   \n",
            "25%        1.287500     -2.362675      0.240305     63.894000    154.874000   \n",
            "50%        1.942890     -1.484200      0.802956     75.437500    172.954000   \n",
            "75%        2.943040     -0.468251      1.694150     84.148450    181.022500   \n",
            "max        6.238690      2.030470      4.496200    121.783000    202.625000   \n",
            "\n",
            "            TC55553       TC55555       TC55566       TI40050       TI52014  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean     180.878169    180.993123    197.350585     73.006564    128.880168   \n",
            "std       27.783800      1.543444     13.038620     14.901401     11.140228   \n",
            "min      110.534000    175.546000    156.727000     27.985400     96.720000   \n",
            "25%      161.732500    179.961000    186.081000     62.530300    121.002000   \n",
            "50%      175.403000    180.109000    200.170000     76.155300    129.270000   \n",
            "75%      194.358500    182.022000    205.158000     84.230750    137.256500   \n",
            "max      246.047000    186.533000    227.515000    105.000000    162.257000   \n",
            "\n",
            "            TI55013       TI55014       TI55015       TI55016       TI55017  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean     200.626470    196.849812    194.540385    189.889625    185.814189   \n",
            "std       17.679004     13.690945     13.581476      9.111545      9.669426   \n",
            "min      161.963000    167.953000    164.345000    172.232000    159.837000   \n",
            "25%      186.434000    186.070000    184.058500    182.661000    178.603000   \n",
            "50%      198.946000    195.699000    193.322000    188.902000    184.853000   \n",
            "75%      209.816500    203.469500    201.369500    194.322500    191.180500   \n",
            "max      240.882000    227.743000    226.311000    212.767000    211.069000   \n",
            "\n",
            "            TI55021       TI55023      VI52558B  \n",
            "count  40127.000000  40127.000000  40127.000000  \n",
            "mean     223.379511    218.166975      2.830012  \n",
            "std       10.256584      3.419686      0.995163  \n",
            "min      207.365000    206.986000      0.644656  \n",
            "25%      217.464000    215.527500      2.127010  \n",
            "50%      220.225000    217.536000      2.648960  \n",
            "75%      223.955500    220.568000      3.336320  \n",
            "max      259.004000    230.025000      6.512480  \n",
            "         425_pct_Al         Al2O3       M_Value    C4_pct_Eth    C4_pct_H2O  \\\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000   \n",
            "mean       6.133712     11.382610      3.597077      1.213861     21.178586   \n",
            "std        0.229069      0.547888      0.180890      0.692503      2.751294   \n",
            "min        5.268290      9.738800      1.253440      0.026531      7.606320   \n",
            "25%        6.003465     11.096400      3.502750      0.621587     18.841700   \n",
            "50%        6.135840     11.495800      3.587960      1.103180     21.462900   \n",
            "75%        6.266050     11.719950      3.682920      1.687660     23.272850   \n",
            "max        7.034650     12.880700      5.831770      9.844330     34.362400   \n",
            "\n",
            "       HydWtr_pct_Ammonia    C4_pct_Hex   HydWtr_Na2O  \n",
            "count        40127.000000  40127.000000  40127.000000  \n",
            "mean             0.967939      0.471405      0.789882  \n",
            "std              0.146906      0.241560      0.689519  \n",
            "min              0.366569      0.000321      0.000000  \n",
            "25%              0.877052      0.353835      0.449473  \n",
            "50%              0.939770      0.446940      0.646366  \n",
            "75%              1.028570      0.546338      0.960590  \n",
            "max              1.615560      3.086970     13.474300  \n",
            "            Butanol       Decanol       Ethanol       Hexanol       Octanol\n",
            "count  40127.000000  40127.000000  40127.000000  40127.000000  40127.000000\n",
            "mean       9.097426      3.114636     19.368811      3.665954      4.270774\n",
            "std        9.024005      1.518655     18.535968      1.948907      2.144554\n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000\n",
            "25%        3.846945      1.960000      5.900000      2.130070      2.592545\n",
            "50%        5.790710      3.032810     13.043500      3.420000      4.165000\n",
            "75%       10.120200      4.120000     26.909300      4.975000      5.750000\n",
            "max       56.913300      7.967800     97.266700     10.680000     10.690000\n"
          ]
        }
      ],
      "source": [
        "print(df_CDunit.describe())\n",
        "print(df_AlCon.describe())\n",
        "print(df_FB554.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: object\n",
            "Data type for 'Date' column in df_FB554: object\n",
            "Data type for 'Date' column in df_AlCon: object\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: datetime64[ns]\n",
            "Data type for 'Date' column in df_FB554: datetime64[ns]\n",
            "Data type for 'Date' column in df_AlCon: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Date', 'AYC55580', 'DI55102', 'DI55152', 'DI55580', 'FC42428',\n",
            "       'FC52018', 'FC55003', 'FC55009', 'FC55102', 'FC55152', 'FC55552',\n",
            "       'FC55555', 'FC55569', 'FC55576', 'FFC55553', 'FFC55555', 'FYC55553',\n",
            "       'II52554', 'LC52572', 'LC55553', 'LC55555', 'LC55557', 'LC55568',\n",
            "       'LC90366', 'LC90368', 'PI55004', 'PI55020', 'PI55560', 'TC52015',\n",
            "       'TC55552', 'TC55553', 'TC55555', 'TC55566', 'TI40050', 'TI52014',\n",
            "       'TI55013', 'TI55014', 'TI55015', 'TI55016', 'TI55017', 'TI55021',\n",
            "       'TI55023', 'VI52558B'],\n",
            "      dtype='object')\n",
            "Index(['Date', 'Butanol', 'Decanol', 'Ethanol', 'Hexanol', 'Octanol'], dtype='object')\n",
            "Index(['Date', '425_pct_Al', 'Al2O3', 'M_Value', 'C4_pct_Eth', 'C4_pct_H2O',\n",
            "       'HydWtr_pct_Ammonia', 'C4_pct_Hex', 'HydWtr_Na2O'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df_CDunit.columns)\n",
        "print(df_FB554.columns)\n",
        "print(df_AlCon.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_rolling_average_to_df(df, rolling_size):\n",
        "    # Ensure 'Date' is the index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Apply rolling average to all columns\n",
        "    rolled_df = df.rolling(window=rolling_size, min_periods=1).mean()\n",
        "\n",
        "    # Reset index to make 'Date' a column again\n",
        "    rolled_df = rolled_df.reset_index()\n",
        "\n",
        "    return rolled_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_time_shift_by_hours(df, shift_hours):\n",
        "    \"\"\"\n",
        "    Shifts the DataFrame's datetime index by the specified number of hours.\n",
        "\n",
        "    :param df: DataFrame with 'Date' as its datetime index or column.\n",
        "    :param shift_hours: Number of hours to shift. Can be positive (forward) or negative (backward).\n",
        "    :return: Shifted DataFrame.\n",
        "    \"\"\"\n",
        "    # Convert 'Date' to datetime and set as index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Ensure the index is a DatetimeIndex\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Shift the DataFrame's index by the specified number of hours\n",
        "    df.index = df.index + pd.Timedelta(hours=shift_hours)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Usage Examples\n",
        "# shift_hours_AlCon = 1  # Negative shift for df_AlCon (e.g., -5 hours backward)\n",
        "# shift_hours_FB554 = 5   # Positive shift for df_FB554 (e.g., 5 hours forward)\n",
        "\n",
        "# shifted_df_AlCon = apply_time_shift_by_hours(df_AlCon, shift_hours_AlCon)\n",
        "# print(\"Shifted df_AlCon:\")\n",
        "# print(shifted_df_AlCon.head())\n",
        "\n",
        "# shifted_df_FB554 = apply_time_shift_by_hours(df_FB554, shift_hours_FB554)\n",
        "# print(\"\\nShifted df_FB554:\")\n",
        "# print(shifted_df_FB554.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_df_FB554_to_df_CDunit(df_CDunit, df_FB554):\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "    df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "\n",
        "    df_CDunit = df_CDunit.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_FB554 = df_FB554.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df = pd.merge_asof(df_FB554, df_CDunit, on='Date', direction='nearest')\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def join_df_AlCon_to_combined_df(combined_df, df_AlCon):\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], errors='coerce')\n",
        "    df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n",
        "\n",
        "    combined_df = combined_df.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_AlCon = df_AlCon.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df_all = pd.merge_asof(df_AlCon, combined_df, on='Date', direction='nearest')\n",
        "    \n",
        "    return combined_df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: datetime64[ns]\n",
            "Data type for 'Date' column in df_FB554: datetime64[ns]\n",
            "Data type for 'Date' column in df_AlCon: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Date'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\austinsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m rolled_df_FB554_shifted \u001b[38;5;241m=\u001b[39m apply_time_shift_by_hours(rolled_df_FB554, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Merge the rolled and shifted DataFrames accordingly\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mjoin_df_FB554_to_df_CDunit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrolled_df_CDunit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrolled_df_FB554_shifted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m combined_df_all \u001b[38;5;241m=\u001b[39m join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon_shifted)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Now, combined_df_all is the DataFrame you're interested in\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36mjoin_df_FB554_to_df_CDunit\u001b[1;34m(df_CDunit, df_FB554)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin_df_FB554_to_df_CDunit\u001b[39m(df_CDunit, df_FB554):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Ensure 'Date' columns are datetime objects and sort DataFrames\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     df_CDunit[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_CDunit[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     df_FB554[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf_FB554\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     df_CDunit \u001b[38;5;241m=\u001b[39m df_CDunit\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     df_FB554 \u001b[38;5;241m=\u001b[39m df_FB554\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\austinsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\austinsh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3807\u001b[0m     ):\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Date'"
          ]
        }
      ],
      "source": [
        "def apply_negative_shift_hours(shift_hours):\n",
        "    return [-hour for hour in shift_hours]\n",
        "\n",
        "# Make sure to have your DataFrames: df_CDunit, df_FB554, df_AlCon ready\n",
        "\n",
        "# Apply the specific rolling average directly\n",
        "rolled_df_CDunit = apply_rolling_average_to_df(df_CDunit, 8)\n",
        "rolled_df_FB554 = apply_rolling_average_to_df(df_FB554, 4)\n",
        "rolled_df_AlCon = apply_rolling_average_to_df(df_AlCon, 2)\n",
        "\n",
        "# Apply the specific time shifts directly\n",
        "# Note: Since you're applying a negative shift to AlCon and a positive shift to FB554, ensure this logic is correctly handled in your apply_time_shift_by_hours function\n",
        "rolled_df_AlCon_shifted = apply_time_shift_by_hours(rolled_df_AlCon, -1) # apply_negative_shift_hours already applied in specifying -1\n",
        "rolled_df_FB554_shifted = apply_time_shift_by_hours(rolled_df_FB554, 1)\n",
        "\n",
        "# Merge the rolled and shifted DataFrames accordingly\n",
        "combined_df = join_df_FB554_to_df_CDunit(rolled_df_CDunit, rolled_df_FB554_shifted)\n",
        "combined_df_all = join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon_shifted)\n",
        "\n",
        "# Now, combined_df_all is the DataFrame you're interested in\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "def apply_negative_shift_hours(shift_hours):\n",
        "    return [-hour for hour in shift_hours]\n",
        "\n",
        "# Rolling sizes ranges\n",
        "rolling_size_CDunit = [8]  # Even rolling sizes from 4 to 10 range(4, 11, 2)\n",
        "rolling_size_FB554 = [4]  # Even rolling sizes from 4 to 10 range(4, 11, 2)\n",
        "rolling_size_AlCon = [2]  # Even rolling sizes from 2 to 30 range(2, 31, 2) \n",
        "\n",
        "# Shift hours ranges\n",
        "shift_hours_AlCon = apply_negative_shift_hours([1])  # Negative shifts from -2 to -8 (range(2, 9, 2)) \n",
        "shift_hours_FB554 = ([1])    # Positive shifts from 2 to 8 range(2, 9, 2)\n",
        "\n",
        "# Precompute rolling averages for each DataFrame and each rolling size\n",
        "precomputed_rolls = {\n",
        "    \"CDunit\": {size: apply_rolling_average_to_df(df_CDunit, size) for size in rolling_size_CDunit},\n",
        "    \"FB554\": {size: apply_rolling_average_to_df(df_FB554, size) for size in rolling_size_FB554},\n",
        "    \"AlCon\": {size: apply_rolling_average_to_df(df_AlCon, size) for size in rolling_size_AlCon}\n",
        "}\n",
        "\n",
        "results = pd.DataFrame()\n",
        "\n",
        "## Modified process_data function\n",
        "def process_data():\n",
        "    iteration_count = 0\n",
        "    results = pd.DataFrame(columns=['Iteration', 'Rolling Sizes CDunit', 'Rolling Sizes FB554', 'Rolling Sizes AlCon',\n",
        "                                    'Shift Hours AlCon', 'Shift Hours FB554', 'R-squared', 'Adj R-Squared', \n",
        "                                    'F-statistic', 'Prob (F-statistic)'])\n",
        "\n",
        "    for size_CDunit in rolling_size_CDunit:\n",
        "        for size_FB554 in rolling_size_FB554:\n",
        "            for size_AlCon in rolling_size_AlCon:\n",
        "                for shift_hour_AlCon in shift_hours_AlCon:\n",
        "                    for shift_hour_FB554 in shift_hours_FB554:\n",
        "                        iteration_count += 1\n",
        "\n",
        "                        # Retrieve rolled dataframes\n",
        "                        rolled_df_CDunit = precomputed_rolls[\"CDunit\"][size_CDunit]\n",
        "                        rolled_df_FB554 = precomputed_rolls[\"FB554\"][size_FB554]\n",
        "                        rolled_df_AlCon = precomputed_rolls[\"AlCon\"][size_AlCon]\n",
        "\n",
        "                        # Combine df_CDunit and df_FB554 to create combined_df\n",
        "                        combined_df = join_df_FB554_to_df_CDunit(rolled_df_CDunit, rolled_df_FB554)\n",
        "\n",
        "                        # Combine combined_df with rolled_df_AlCon to create combined_df_all\n",
        "                        combined_df_all = join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon)\n",
        "\n",
        "                        # Drop 'Date' column before modeling\n",
        "                        combined_df_all = combined_df_all.drop(columns=['Date'], errors='ignore')\n",
        "\n",
        "                        # Splitting into train and test\n",
        "                        X = combined_df_all.drop('Butanol', axis=1)\n",
        "                        y = combined_df_all['Butanol']\n",
        "                        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                        # Train model\n",
        "                        model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "                        # Store the results instead of printing\n",
        "                        iteration_results = {\n",
        "                            'Iteration': iteration_count,\n",
        "                            'Rolling Sizes CDunit': size_CDunit,\n",
        "                            'Rolling Sizes FB554': size_FB554,\n",
        "                            'Rolling Sizes AlCon': size_AlCon,\n",
        "                            'Shift Hours AlCon': shift_hour_AlCon,\n",
        "                            'Shift Hours FB554': shift_hour_FB554,\n",
        "                            'R-squared': model.rsquared,\n",
        "                            'Adj R-Squared': model.rsquared_adj,\n",
        "                            'F-statistic': model.fvalue,\n",
        "                            'Prob (F-statistic)': model.f_pvalue\n",
        "                        }\n",
        "                        # 1 results = results.append(iteration_results, ignore_index=True)?\n",
        "                        # 2 results = pd.concat([results, iteration_results], ignore_index=True)\n",
        "                        # iteration_results_df = pd.DataFrame([iteration_results])\n",
        "                        # results = pd.concat([results], ignore_index=True) #, iteration_results_df\n",
        "                        iteration_results_df = pd.DataFrame([iteration_results])\n",
        "                        results = pd.concat([results, iteration_results_df], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "                        # Print only the iteration count\n",
        "                        print(f\"Iteration: {iteration_count}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Call the function to process and evaluate the data\n",
        "model_results = process_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "# model_results.to_csv('merged_data'.csv', index=False)\n",
        "                     \n",
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "# df_CD.to_csv(r'C:\\Users\\steve\\OneDrive\\1. BAIUTEK\\Project-OptiC4\\1 Preprocess\\Continuous Data\\contData_all.csv', index=False)            \n",
        "\n",
        "model_results.to_csv(r'C:\\Users\\austinsh\\Project-OptiC4\\1 Preprocess\\Merge Data\\merged_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get the current date and time\n",
        "current_date_time = datetime.now()\n",
        "\n",
        "# Print the current date and time\n",
        "print(current_date_time)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
