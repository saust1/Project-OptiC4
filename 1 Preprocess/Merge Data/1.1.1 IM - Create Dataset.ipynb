{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Set max columns to display\n",
        "pd.set_option('display.max_columns', None)\n",
        "from sklearn import preprocessing as pre\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importing CSV files\n",
        "df_CDunit = pd.read_csv(r\"C:\\Users\\saust\\OneDrive\\Desktop\\GitRepo\\Project-OptiC4\\1 Preprocess\\Continuous Data\\cont_554Data_clean.csv\")\n",
        "df_AlCon = pd.read_csv(r\"C:\\Users\\saust\\OneDrive\\Desktop\\GitRepo\\Project-OptiC4\\1 Preprocess\\Continuous Data\\cont_425Data_clean.csv\")\n",
        "df_FB554 = pd.read_csv(r\"C:\\Users\\saust\\OneDrive\\Desktop\\GitRepo\\Project-OptiC4\\1 Preprocess\\Continuous Data\\cont_unitData_clean.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            Butanol\n",
            "count  54012.000000\n",
            "mean       9.616727\n",
            "std       10.163612\n",
            "min        0.000000\n",
            "25%        3.737802\n",
            "50%        5.793330\n",
            "75%       10.633775\n",
            "max       58.722400\n",
            "            425 %Al\n",
            "count  54012.000000\n",
            "mean       6.131241\n",
            "std        0.258970\n",
            "min        5.089190\n",
            "25%        5.991428\n",
            "50%        6.133225\n",
            "75%        6.274580\n",
            "max        7.173480\n",
            "            DI55102       DI55152       DI55580       FC42428       FC55003  \\\n",
            "count  54012.000000  54012.000000  54012.000000  54012.000000  54012.000000   \n",
            "mean       0.929994      0.941750      0.997958  35836.107730   5952.945003   \n",
            "std        0.067254      0.039547      0.049932   5224.810968    848.116136   \n",
            "min        0.749916      0.817618      0.844666  16329.300000   2835.740000   \n",
            "25%        0.892624      0.915336      0.966470  33853.275000   5418.517500   \n",
            "50%        0.940790      0.938520      0.997832  37707.500000   5971.345000   \n",
            "75%        0.980906      0.962228      1.032240  39361.125000   6524.025000   \n",
            "max        1.068230      1.065010      1.144600  49959.600000   9052.540000   \n",
            "\n",
            "            FC55552       FC55569       FC55576      FFC55553      FFC55555  \\\n",
            "count  54012.000000  54012.000000  54012.000000  54012.000000  54012.000000   \n",
            "mean   36272.412858   6597.893189    376.305297      0.994843      0.774018   \n",
            "std     4292.665558    368.996191    255.777648      0.040149      0.023266   \n",
            "min    18922.400000   5167.460000      0.000000      0.811764      0.694477   \n",
            "25%    35091.675000   6371.432500    202.940000      0.970274      0.759399   \n",
            "50%    37938.800000   6578.000000    348.692500      0.993260      0.777864   \n",
            "75%    38994.725000   6812.530000    543.986000      1.016520      0.789928   \n",
            "max    46356.900000   8014.950000   1163.020000      1.170060      0.853458   \n",
            "\n",
            "            LC52572       LC55553       LC55555       LC55557       LC55568  \\\n",
            "count  54012.000000  54012.000000  54012.000000  54012.000000  54012.000000   \n",
            "mean      63.074773     63.420477     58.060675     69.764047     41.077603   \n",
            "std        2.600417      6.917031     10.593966      3.042409      1.501380   \n",
            "min       53.483600     42.429300     26.868600     60.006600     32.353800   \n",
            "25%       61.733825     59.996950     49.531475     67.896175     40.557600   \n",
            "50%       63.343800     65.341300     60.028950     70.117550     41.202450   \n",
            "75%       64.906300     68.096425     67.007525     71.911900     41.577200   \n",
            "max       72.423800     83.548700     82.809000     79.508200     49.255100   \n",
            "\n",
            "            LC90366       LC90368       PI55004       PI55020       TC55552  \\\n",
            "count  54012.000000  54012.000000  54012.000000  54012.000000  54012.000000   \n",
            "mean      47.060293     38.466543      2.241804     -1.474149    168.736278   \n",
            "std       28.049958     20.408456      1.201207      1.107205     15.190144   \n",
            "min        0.000000      0.006367      0.001366     -4.803710    120.273000   \n",
            "25%       23.257500     23.243800      1.285995     -2.321870    156.125000   \n",
            "50%       49.849150     44.414350      2.004730     -1.459085    172.436000   \n",
            "75%       73.275400     53.735625      3.029292     -0.493724    180.854250   \n",
            "max       87.901600     81.210800      6.333520      1.983020    207.426000   \n",
            "\n",
            "            TC55553       TC55555  \n",
            "count  54012.000000  54012.000000  \n",
            "mean     182.540058    181.001699  \n",
            "std       28.275171      1.497583  \n",
            "min      110.245000    175.660000  \n",
            "25%      162.161750    179.968000  \n",
            "50%      176.690000    180.125000  \n",
            "75%      201.563500    182.003000  \n",
            "max      261.360000    186.494000  \n"
          ]
        }
      ],
      "source": [
        "print(df_CDunit.describe())\n",
        "print(df_AlCon.describe())\n",
        "print(df_FB554.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: object\n",
            "Data type for 'Date' column in df_FB554: object\n",
            "Data type for 'Date' column in df_AlCon: object\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: datetime64[ns]\n",
            "Data type for 'Date' column in df_FB554: datetime64[ns]\n",
            "Data type for 'Date' column in df_AlCon: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Date', 'Butanol'], dtype='object')\n",
            "Index(['Date', 'DI55102', 'DI55152', 'DI55580', 'FC42428', 'FC55003',\n",
            "       'FC55552', 'FC55569', 'FC55576', 'FFC55553', 'FFC55555', 'LC52572',\n",
            "       'LC55553', 'LC55555', 'LC55557', 'LC55568', 'LC90366', 'LC90368',\n",
            "       'PI55004', 'PI55020', 'TC55552', 'TC55553', 'TC55555'],\n",
            "      dtype='object')\n",
            "Index(['Date', '425 %Al'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(df_CDunit.columns)\n",
        "print(df_FB554.columns)\n",
        "print(df_AlCon.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_rolling_average_to_df(df, rolling_size):\n",
        "    # Ensure 'Date' is the index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Apply rolling average to all columns\n",
        "    rolled_df = df.rolling(window=rolling_size, min_periods=1).mean()\n",
        "\n",
        "    # Reset index to make 'Date' a column again\n",
        "    rolled_df = rolled_df.reset_index()\n",
        "\n",
        "    return rolled_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_time_shift_by_hours(df, shift_hours):\n",
        "    \"\"\"\n",
        "    Shifts the DataFrame's datetime index by the specified number of hours.\n",
        "\n",
        "    :param df: DataFrame with 'Date' as its datetime index or column.\n",
        "    :param shift_hours: Number of hours to shift. Can be positive (forward) or negative (backward).\n",
        "    :return: Shifted DataFrame.\n",
        "    \"\"\"\n",
        "    # Convert 'Date' to datetime and set as index if it's not already\n",
        "    if df.index.name != 'Date':\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df = df.set_index('Date')\n",
        "\n",
        "    # Ensure the index is a DatetimeIndex\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Shift the DataFrame's index by the specified number of hours\n",
        "    df.index = df.index + pd.Timedelta(hours=shift_hours)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Usage Examples\n",
        "# shift_hours_AlCon = 1  # Negative shift for df_AlCon (e.g., -5 hours backward)\n",
        "# shift_hours_FB554 = 5   # Positive shift for df_FB554 (e.g., 5 hours forward)\n",
        "\n",
        "# shifted_df_AlCon = apply_time_shift_by_hours(df_AlCon, shift_hours_AlCon)\n",
        "# print(\"Shifted df_AlCon:\")\n",
        "# print(shifted_df_AlCon.head())\n",
        "\n",
        "# shifted_df_FB554 = apply_time_shift_by_hours(df_FB554, shift_hours_FB554)\n",
        "# print(\"\\nShifted df_FB554:\")\n",
        "# print(shifted_df_FB554.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def join_df_FB554_to_df_CDunit(df_CDunit, df_FB554):\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    df_CDunit['Date'] = pd.to_datetime(df_CDunit['Date'], errors='coerce')\n",
        "    df_FB554['Date'] = pd.to_datetime(df_FB554['Date'], errors='coerce')\n",
        "\n",
        "    df_CDunit = df_CDunit.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_FB554 = df_FB554.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df = pd.merge_asof(df_FB554, df_CDunit, on='Date', direction='nearest')\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "def join_df_AlCon_to_combined_df(combined_df, df_AlCon):\n",
        "    # Ensure 'Date' columns are datetime objects and sort DataFrames\n",
        "    combined_df['Date'] = pd.to_datetime(combined_df['Date'], errors='coerce')\n",
        "    df_AlCon['Date'] = pd.to_datetime(df_AlCon['Date'], errors='coerce')\n",
        "\n",
        "    combined_df = combined_df.dropna(subset=['Date']).sort_values('Date')\n",
        "    df_AlCon = df_AlCon.dropna(subset=['Date']).sort_values('Date')\n",
        "\n",
        "    # Perform merge_asof\n",
        "    combined_df_all = pd.merge_asof(df_AlCon, combined_df, on='Date', direction='nearest')\n",
        "    \n",
        "    return combined_df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data type for 'Date' column in df_CDunit: datetime64[ns]\n",
            "Data type for 'Date' column in df_FB554: datetime64[ns]\n",
            "Data type for 'Date' column in df_AlCon: datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(\"Data type for 'Date' column in df_CDunit:\", df_CDunit['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_FB554:\", df_FB554['Date'].dtypes)\n",
        "print(\"Data type for 'Date' column in df_AlCon:\", df_AlCon['Date'].dtypes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iterative merger \n",
        "# avereaged F stat\n",
        "# filtered two closest f stats by whole number\n",
        "# Took the best adj R^2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\saust\\AppData\\Local\\Temp\\ipykernel_98580\\2705700377.py:73: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results = results.append(iteration_results, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "def apply_negative_shift_hours(shift_hours):\n",
        "    return [-hour for hour in shift_hours]\n",
        "\n",
        "# Rolling sizes ranges\n",
        "rolling_size_CDunit = [8]  # Even rolling sizes from 4 to 10 range(4, 11, 2)\n",
        "rolling_size_FB554 = [4]  # Even rolling sizes from 4 to 10 range(4, 11, 2)\n",
        "rolling_size_AlCon = [2]  # Even rolling sizes from 2 to 30 range(2, 31, 2) \n",
        "\n",
        "# Shift hours ranges\n",
        "shift_hours_AlCon = apply_negative_shift_hours([1])  # Negative shifts from -2 to -8 (range(2, 9, 2)) \n",
        "shift_hours_FB554 = ([1])    # Positive shifts from 2 to 8 range(2, 9, 2)\n",
        "\n",
        "# Precompute rolling averages for each DataFrame and each rolling size\n",
        "precomputed_rolls = {\n",
        "    \"CDunit\": {size: apply_rolling_average_to_df(df_CDunit, size) for size in rolling_size_CDunit},\n",
        "    \"FB554\": {size: apply_rolling_average_to_df(df_FB554, size) for size in rolling_size_FB554},\n",
        "    \"AlCon\": {size: apply_rolling_average_to_df(df_AlCon, size) for size in rolling_size_AlCon}\n",
        "}\n",
        "\n",
        "## Modified process_data function\n",
        "def process_data():\n",
        "    iteration_count = 0\n",
        "    results = pd.DataFrame(columns=['Iteration', 'Rolling Sizes CDunit', 'Rolling Sizes FB554', 'Rolling Sizes AlCon',\n",
        "                                    'Shift Hours AlCon', 'Shift Hours FB554', 'R-squared', 'Adj R-Squared', \n",
        "                                    'F-statistic', 'Prob (F-statistic)'])\n",
        "    all_combined_dfs = []  # List to store combined_df_all for each iteration\n",
        "\n",
        "    for size_CDunit in rolling_size_CDunit:\n",
        "        for size_FB554 in rolling_size_FB554:\n",
        "            for size_AlCon in rolling_size_AlCon:\n",
        "                for shift_hour_AlCon in shift_hours_AlCon:\n",
        "                    for shift_hour_FB554 in shift_hours_FB554:\n",
        "                        iteration_count += 1\n",
        "\n",
        "                        # Retrieve rolled dataframes\n",
        "                        rolled_df_CDunit = precomputed_rolls[\"CDunit\"][size_CDunit]\n",
        "                        rolled_df_FB554 = precomputed_rolls[\"FB554\"][size_FB554]\n",
        "                        rolled_df_AlCon = precomputed_rolls[\"AlCon\"][size_AlCon]\n",
        "\n",
        "                        # Combine df_CDunit and df_FB554 to create combined_df\n",
        "                        combined_df = join_df_FB554_to_df_CDunit(rolled_df_CDunit, rolled_df_FB554)\n",
        "\n",
        "                        # Combine combined_df with rolled_df_AlCon to create combined_df_all\n",
        "                        combined_df_all = join_df_AlCon_to_combined_df(combined_df, rolled_df_AlCon)\n",
        "\n",
        "                        # Drop 'Date' column before modeling\n",
        "                        combined_df_all = combined_df_all.drop(columns=['Date'], errors='ignore')\n",
        "\n",
        "                        # Store combined_df_all in the list\n",
        "                        all_combined_dfs.append(combined_df_all)\n",
        "\n",
        "                        # Splitting into train and test\n",
        "                        X = combined_df_all.drop('Butanol', axis=1)\n",
        "                        y = combined_df_all['Butanol']\n",
        "                        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "                        # Train model\n",
        "                        model = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "                        # Store the results instead of printing\n",
        "                        iteration_results = {\n",
        "                            'Iteration': iteration_count,\n",
        "                            'Rolling Sizes CDunit': size_CDunit,\n",
        "                            'Rolling Sizes FB554': size_FB554,\n",
        "                            'Rolling Sizes AlCon': size_AlCon,\n",
        "                            'Shift Hours AlCon': shift_hour_AlCon,\n",
        "                            'Shift Hours FB554': shift_hour_FB554,\n",
        "                            'R-squared': model.rsquared,\n",
        "                            'Adj R-Squared': model.rsquared_adj,\n",
        "                            'F-statistic': model.fvalue,\n",
        "                            'Prob (F-statistic)': model.f_pvalue\n",
        "                        }\n",
        "                        results = results.append(iteration_results, ignore_index=True)\n",
        "\n",
        "                        # Print only the iteration count\n",
        "                        print(f\"Iteration: {iteration_count}\")\n",
        "\n",
        "    return results, all_combined_dfs  # Return both results and the list of combined_df_all\n",
        "\n",
        "# Call the function to process and evaluate the data\n",
        "model_results, all_combined_dfs = process_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save DataFrame to CSV file in the same directory as the Jupyter Notebook\n",
        "model_results.to_csv('model_results2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_df_all = pd.concat(all_combined_dfs)\n",
        "\n",
        "# Assuming combined_df_all is your DataFrame\n",
        "\n",
        "# Calculate the index to split the DataFrame\n",
        "split_index = len(combined_df_all) // 2\n",
        "\n",
        "# Split the DataFrame into two parts\n",
        "df_part1 = combined_df_all.iloc[:split_index]\n",
        "df_part2 = combined_df_all.iloc[split_index:]\n",
        "\n",
        "# Save the two parts to CSV\n",
        "df_part1.to_csv('contData_all_Avg_1o2.csv', index=False)\n",
        "df_part2.to_csv('contData_all_Avg_2o2.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
